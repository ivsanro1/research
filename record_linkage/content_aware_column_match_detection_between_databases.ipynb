{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08724f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow URI: file:///root/work/research/record_linkage/experiments/record_linkage/mlflow/mlruns\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Override/set credentials in env var\n",
    "os.environ['CWD'] = str(Path(os.getcwd()))\n",
    "# os.environ['CWD'] = os.environ['CWD'].parent\n",
    "\n",
    "# Base paths\n",
    "cwd = Path(os.environ['CWD'])\n",
    "dir_data = cwd / 'data'\n",
    "\n",
    "# Set mlflow artifacts location\n",
    "dir_exp_root = Path(cwd / 'experiments')\n",
    "dir_exp_this = Path(dir_exp_root / 'record_linkage')\n",
    "dir_exp_mlflow = dir_exp_this / 'mlflow'\n",
    "dir_exp_mlflow.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "import mlflow\n",
    "mlflow_tracking_uri = (dir_exp_mlflow / 'mlruns').as_uri()\n",
    "print(f'mlflow URI: {mlflow_tracking_uri}')\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a740d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyspark[pandas_on_spark]==3.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c6cf3",
   "metadata": {},
   "source": [
    "# Load fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff6a2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = pd.read_csv('db1.csv')\n",
    "db2 = pd.read_csv('db2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "af4d58ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>job</th>\n",
       "      <th>company</th>\n",
       "      <th>ssn</th>\n",
       "      <th>residence</th>\n",
       "      <th>current_location</th>\n",
       "      <th>blood_group</th>\n",
       "      <th>website</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>address</th>\n",
       "      <th>mail</th>\n",
       "      <th>birthdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Herbalist</td>\n",
       "      <td>Norton-Castillo</td>\n",
       "      <td>049-66-7039</td>\n",
       "      <td>9727 Todd Unions\\nStaceyville, MN 41947</td>\n",
       "      <td>(Decimal('-55.9124455'), Decimal('32.664240'))</td>\n",
       "      <td>AB-</td>\n",
       "      <td>['https://butler.info/', 'http://johnson.net/']</td>\n",
       "      <td>shannon98</td>\n",
       "      <td>Amanda Arroyo</td>\n",
       "      <td>F</td>\n",
       "      <td>0751 Samantha Walk Apt. 650\\nChenfort, SC 04033</td>\n",
       "      <td>schroedermelissa@yahoo.com</td>\n",
       "      <td>1970-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Outdoor activities/education manager</td>\n",
       "      <td>Bowman-Jensen</td>\n",
       "      <td>067-35-7522</td>\n",
       "      <td>Unit 0723 Box 5155\\nDPO AP 51042</td>\n",
       "      <td>(Decimal('19.764603'), Decimal('67.662516'))</td>\n",
       "      <td>A-</td>\n",
       "      <td>['http://www.parker.com/', 'https://www.lowe.i...</td>\n",
       "      <td>nathaniel53</td>\n",
       "      <td>Victoria Brown</td>\n",
       "      <td>F</td>\n",
       "      <td>787 Alexander Road\\nPort Leslieborough, VA 53325</td>\n",
       "      <td>tylermonica@hotmail.com</td>\n",
       "      <td>1926-06-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Chemist, analytical</td>\n",
       "      <td>Wilkerson, Guerrero and Mason</td>\n",
       "      <td>219-24-2883</td>\n",
       "      <td>878 Charles Mountain\\nNorth Loriton, LA 14794</td>\n",
       "      <td>(Decimal('-57.0475775'), Decimal('4.464624'))</td>\n",
       "      <td>AB+</td>\n",
       "      <td>['https://www.salinas.com/', 'https://kennedy....</td>\n",
       "      <td>uperry</td>\n",
       "      <td>Amy Henry</td>\n",
       "      <td>F</td>\n",
       "      <td>706 Sarah Lakes Apt. 421\\nSouth Jeremy, AR 49313</td>\n",
       "      <td>pdelgado@hotmail.com</td>\n",
       "      <td>2016-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Geneticist, molecular</td>\n",
       "      <td>Simmons-Martinez</td>\n",
       "      <td>383-07-2464</td>\n",
       "      <td>993 Boyd Throughway\\nNew Paul, WI 12980</td>\n",
       "      <td>(Decimal('-6.687283'), Decimal('110.936355'))</td>\n",
       "      <td>B+</td>\n",
       "      <td>['http://wilson-cohen.com/', 'https://riggs-al...</td>\n",
       "      <td>qrobertson</td>\n",
       "      <td>Christopher Curtis</td>\n",
       "      <td>M</td>\n",
       "      <td>7274 Bird Canyon Suite 720\\nValentinechester, ...</td>\n",
       "      <td>patrick79@gmail.com</td>\n",
       "      <td>1966-11-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Structural engineer</td>\n",
       "      <td>Castillo, Clark and Kemp</td>\n",
       "      <td>296-65-4080</td>\n",
       "      <td>35200 Miller Mountain Apt. 251\\nRileyville, MO...</td>\n",
       "      <td>(Decimal('-56.0627945'), Decimal('64.691982'))</td>\n",
       "      <td>B+</td>\n",
       "      <td>['https://baker.com/']</td>\n",
       "      <td>kfitzgerald</td>\n",
       "      <td>Kevin Vargas</td>\n",
       "      <td>M</td>\n",
       "      <td>1310 Anderson Fork Apt. 598\\nBrandonbury, WV 3...</td>\n",
       "      <td>rickytaylor@hotmail.com</td>\n",
       "      <td>1968-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>999995</td>\n",
       "      <td>Radiographer, therapeutic</td>\n",
       "      <td>Baker Inc</td>\n",
       "      <td>259-97-2454</td>\n",
       "      <td>7915 Dalton Lodge\\nWest Melissa, ME 06907</td>\n",
       "      <td>(Decimal('45.6559475'), Decimal('74.918465'))</td>\n",
       "      <td>B+</td>\n",
       "      <td>['http://www.jones-snyder.info/', 'http://www....</td>\n",
       "      <td>daniel20</td>\n",
       "      <td>Frank Brown</td>\n",
       "      <td>M</td>\n",
       "      <td>935 Carter Shoals Apt. 167\\nMartinezport, NC 7...</td>\n",
       "      <td>kenneth41@gmail.com</td>\n",
       "      <td>1972-06-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>999996</td>\n",
       "      <td>Therapist, nutritional</td>\n",
       "      <td>Wilson Inc</td>\n",
       "      <td>741-53-2960</td>\n",
       "      <td>192 Smith Village\\nBiancabury, SD 44689</td>\n",
       "      <td>(Decimal('66.9223325'), Decimal('-50.131176'))</td>\n",
       "      <td>B+</td>\n",
       "      <td>['http://www.alexander.net/', 'https://smith.c...</td>\n",
       "      <td>davisphillip</td>\n",
       "      <td>Stephen Cannon</td>\n",
       "      <td>M</td>\n",
       "      <td>0748 Christina Flats\\nEast Robertburgh, ID 15848</td>\n",
       "      <td>amandachan@yahoo.com</td>\n",
       "      <td>1936-07-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>999997</td>\n",
       "      <td>Make</td>\n",
       "      <td>Young and Sons</td>\n",
       "      <td>627-73-2836</td>\n",
       "      <td>0630 Morgan Plain Suite 156\\nCoxborough, ID 14780</td>\n",
       "      <td>(Decimal('-88.3733525'), Decimal('-27.510056'))</td>\n",
       "      <td>AB-</td>\n",
       "      <td>['https://www.gutierrez.com/']</td>\n",
       "      <td>timothybryan</td>\n",
       "      <td>Sharon Rice</td>\n",
       "      <td>F</td>\n",
       "      <td>42873 Moore Fords Apt. 735\\nAshleymouth, CT 46802</td>\n",
       "      <td>weissdeborah@gmail.com</td>\n",
       "      <td>1997-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>999998</td>\n",
       "      <td>Restaurant manager</td>\n",
       "      <td>Pearson-Fisher</td>\n",
       "      <td>749-52-4350</td>\n",
       "      <td>4546 Banks Haven\\nLake Christinaview, MT 72487</td>\n",
       "      <td>(Decimal('28.7827685'), Decimal('-32.938448'))</td>\n",
       "      <td>AB-</td>\n",
       "      <td>['https://www.jones.com/', 'https://www.rodrig...</td>\n",
       "      <td>brookssharon</td>\n",
       "      <td>Amanda Meyers</td>\n",
       "      <td>F</td>\n",
       "      <td>56524 Joseph Stream Apt. 499\\nPort Christopher...</td>\n",
       "      <td>pjenkins@hotmail.com</td>\n",
       "      <td>1947-08-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>999999</td>\n",
       "      <td>Museum/gallery exhibitions officer</td>\n",
       "      <td>Proctor, Goodwin and Clark</td>\n",
       "      <td>589-88-0122</td>\n",
       "      <td>3015 Smith Groves Apt. 500\\nDanielfort, NC 19158</td>\n",
       "      <td>(Decimal('-79.685446'), Decimal('-119.214727'))</td>\n",
       "      <td>AB-</td>\n",
       "      <td>['http://lewis.info/', 'http://www.powell.com/']</td>\n",
       "      <td>debrapearson</td>\n",
       "      <td>Robert Johnson</td>\n",
       "      <td>M</td>\n",
       "      <td>93171 Melissa Viaduct\\nWest Jamesberg, DC 37406</td>\n",
       "      <td>zjones@gmail.com</td>\n",
       "      <td>1937-08-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                   job  \\\n",
       "0                0                             Herbalist   \n",
       "1                1  Outdoor activities/education manager   \n",
       "2                2                   Chemist, analytical   \n",
       "3                3                 Geneticist, molecular   \n",
       "4                4                   Structural engineer   \n",
       "...            ...                                   ...   \n",
       "999995      999995             Radiographer, therapeutic   \n",
       "999996      999996                Therapist, nutritional   \n",
       "999997      999997                                  Make   \n",
       "999998      999998                    Restaurant manager   \n",
       "999999      999999    Museum/gallery exhibitions officer   \n",
       "\n",
       "                              company          ssn  \\\n",
       "0                     Norton-Castillo  049-66-7039   \n",
       "1                       Bowman-Jensen  067-35-7522   \n",
       "2       Wilkerson, Guerrero and Mason  219-24-2883   \n",
       "3                    Simmons-Martinez  383-07-2464   \n",
       "4            Castillo, Clark and Kemp  296-65-4080   \n",
       "...                               ...          ...   \n",
       "999995                      Baker Inc  259-97-2454   \n",
       "999996                     Wilson Inc  741-53-2960   \n",
       "999997                 Young and Sons  627-73-2836   \n",
       "999998                 Pearson-Fisher  749-52-4350   \n",
       "999999     Proctor, Goodwin and Clark  589-88-0122   \n",
       "\n",
       "                                                residence  \\\n",
       "0                 9727 Todd Unions\\nStaceyville, MN 41947   \n",
       "1                        Unit 0723 Box 5155\\nDPO AP 51042   \n",
       "2           878 Charles Mountain\\nNorth Loriton, LA 14794   \n",
       "3                 993 Boyd Throughway\\nNew Paul, WI 12980   \n",
       "4       35200 Miller Mountain Apt. 251\\nRileyville, MO...   \n",
       "...                                                   ...   \n",
       "999995          7915 Dalton Lodge\\nWest Melissa, ME 06907   \n",
       "999996            192 Smith Village\\nBiancabury, SD 44689   \n",
       "999997  0630 Morgan Plain Suite 156\\nCoxborough, ID 14780   \n",
       "999998     4546 Banks Haven\\nLake Christinaview, MT 72487   \n",
       "999999   3015 Smith Groves Apt. 500\\nDanielfort, NC 19158   \n",
       "\n",
       "                                       current_location blood_group  \\\n",
       "0        (Decimal('-55.9124455'), Decimal('32.664240'))         AB-   \n",
       "1          (Decimal('19.764603'), Decimal('67.662516'))          A-   \n",
       "2         (Decimal('-57.0475775'), Decimal('4.464624'))         AB+   \n",
       "3         (Decimal('-6.687283'), Decimal('110.936355'))          B+   \n",
       "4        (Decimal('-56.0627945'), Decimal('64.691982'))          B+   \n",
       "...                                                 ...         ...   \n",
       "999995    (Decimal('45.6559475'), Decimal('74.918465'))          B+   \n",
       "999996   (Decimal('66.9223325'), Decimal('-50.131176'))          B+   \n",
       "999997  (Decimal('-88.3733525'), Decimal('-27.510056'))         AB-   \n",
       "999998   (Decimal('28.7827685'), Decimal('-32.938448'))         AB-   \n",
       "999999  (Decimal('-79.685446'), Decimal('-119.214727'))         AB-   \n",
       "\n",
       "                                                  website      username  \\\n",
       "0         ['https://butler.info/', 'http://johnson.net/']     shannon98   \n",
       "1       ['http://www.parker.com/', 'https://www.lowe.i...   nathaniel53   \n",
       "2       ['https://www.salinas.com/', 'https://kennedy....        uperry   \n",
       "3       ['http://wilson-cohen.com/', 'https://riggs-al...    qrobertson   \n",
       "4                                  ['https://baker.com/']   kfitzgerald   \n",
       "...                                                   ...           ...   \n",
       "999995  ['http://www.jones-snyder.info/', 'http://www....      daniel20   \n",
       "999996  ['http://www.alexander.net/', 'https://smith.c...  davisphillip   \n",
       "999997                     ['https://www.gutierrez.com/']  timothybryan   \n",
       "999998  ['https://www.jones.com/', 'https://www.rodrig...  brookssharon   \n",
       "999999   ['http://lewis.info/', 'http://www.powell.com/']  debrapearson   \n",
       "\n",
       "                      name sex  \\\n",
       "0            Amanda Arroyo   F   \n",
       "1           Victoria Brown   F   \n",
       "2                Amy Henry   F   \n",
       "3       Christopher Curtis   M   \n",
       "4             Kevin Vargas   M   \n",
       "...                    ...  ..   \n",
       "999995         Frank Brown   M   \n",
       "999996      Stephen Cannon   M   \n",
       "999997         Sharon Rice   F   \n",
       "999998       Amanda Meyers   F   \n",
       "999999      Robert Johnson   M   \n",
       "\n",
       "                                                  address  \\\n",
       "0         0751 Samantha Walk Apt. 650\\nChenfort, SC 04033   \n",
       "1        787 Alexander Road\\nPort Leslieborough, VA 53325   \n",
       "2        706 Sarah Lakes Apt. 421\\nSouth Jeremy, AR 49313   \n",
       "3       7274 Bird Canyon Suite 720\\nValentinechester, ...   \n",
       "4       1310 Anderson Fork Apt. 598\\nBrandonbury, WV 3...   \n",
       "...                                                   ...   \n",
       "999995  935 Carter Shoals Apt. 167\\nMartinezport, NC 7...   \n",
       "999996   0748 Christina Flats\\nEast Robertburgh, ID 15848   \n",
       "999997  42873 Moore Fords Apt. 735\\nAshleymouth, CT 46802   \n",
       "999998  56524 Joseph Stream Apt. 499\\nPort Christopher...   \n",
       "999999    93171 Melissa Viaduct\\nWest Jamesberg, DC 37406   \n",
       "\n",
       "                              mail   birthdate  \n",
       "0       schroedermelissa@yahoo.com  1970-03-09  \n",
       "1          tylermonica@hotmail.com  1926-06-24  \n",
       "2             pdelgado@hotmail.com  2016-03-18  \n",
       "3              patrick79@gmail.com  1966-11-20  \n",
       "4          rickytaylor@hotmail.com  1968-07-01  \n",
       "...                            ...         ...  \n",
       "999995         kenneth41@gmail.com  1972-06-22  \n",
       "999996        amandachan@yahoo.com  1936-07-14  \n",
       "999997      weissdeborah@gmail.com  1997-03-04  \n",
       "999998        pjenkins@hotmail.com  1947-08-16  \n",
       "999999            zjones@gmail.com  1937-08-20  \n",
       "\n",
       "[1000000 rows x 14 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90e1969",
   "metadata": {},
   "source": [
    "# OR: Generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f00e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 1000000/1000000 [18:29<00:00, 901.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from faker import Faker\n",
    "Faker.seed(288)\n",
    "fake = Faker(\n",
    "#     [\n",
    "#     'it_IT',\n",
    "#     'en_US',\n",
    "#     'es_ES',\n",
    "#     'en_CA'\n",
    "#     ]\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "db1 = pd.DataFrame([fake.profile() for _ in tqdm(list(range(1000000)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6c9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db1.to_csv('db1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbfb779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96557ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add current_location\n",
    "# Remove fields randomly \n",
    "# db1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best config found:\n",
    "# Min df: 50 \n",
    "# Max df: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a35a8cf",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "P_NOISE_CHAR = 0.1\n",
    "NUM_TOKENS_KEPT_ADDR = 2\n",
    "ROW_COL_MISSING_OR_SWAPPED = None #'SWAP' # None, 'MISSING'\n",
    "P_ROW_COL_MISSING_OR_SWAPPED = 1\n",
    "FRAC_KEPT_ROWS_DB2 = 0.5\n",
    "\n",
    "\n",
    "TFIDF_ANALYZER = 'char_wb'\n",
    "TFIDF_NGRAM_LO = 3\n",
    "TFIDF_NGRAM_HI = 3\n",
    "TFIDF_MAX_DF = 0.8\n",
    "TFIDF_MIN_DF = 10\n",
    "TFIDF_MAX_FEATS = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c944b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "np.random.seed(288)\n",
    "\n",
    "db2 = db1[['job', 'address', 'name']]\n",
    "\n",
    "import re\n",
    "def _split(txt):\n",
    "    return [x.strip() for x in re.split('-|\\s| and |,', txt) if x.strip() != '']\n",
    "\n",
    "# _split('Wilson, Sanchez and Pearson')\n",
    "\n",
    "db2['job'] = db2['job'].apply(lambda txt: \n",
    "    np.random.choice(_split(txt))\n",
    ")\n",
    "\n",
    "db2['name'] = db2['name'].apply(lambda txt: \n",
    "    np.random.choice(_split(txt))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "db2['address'] = db2['address'].apply(lambda txt: \n",
    "    ' '.join(np.random.choice(_split(txt), size=min(NUM_TOKENS_KEPT_ADDR, len(_split(txt))), replace=False)) # Keep 2 tokens when possible, otherwise 1\n",
    ")\n",
    "\n",
    "import string\n",
    "\n",
    "def _add_char_noise(txt, p, noise_set_chars=[c for c in string.ascii_lowercase + string.digits]):\n",
    "    txt_noise = ''\n",
    "    for c in txt:\n",
    "        if np.random.rand() < p:\n",
    "            txt_noise += np.random.choice(noise_set_chars)\n",
    "        else:\n",
    "            txt_noise += c\n",
    "    return txt_noise\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "f_add_char_noise = partial(_add_char_noise, p=P_NOISE_CHAR)\n",
    "\n",
    "\n",
    "\n",
    "db2['job'] = db2['job'].apply(f_add_char_noise)\n",
    "db2['address'] = db2['address'].apply(f_add_char_noise)\n",
    "db2['name'] = db2['name'].apply(f_add_char_noise)\n",
    "\n",
    "\n",
    "_prime = '′'\n",
    "d_name_map_c1c2 = {k:k+_prime for k in ['name', 'address', 'job']}\n",
    "db2 = db2.rename(columns=d_name_map_c1c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd14ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c01dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_swap_columns(row, subset, p):\n",
    "    if np.random.rand() < p:\n",
    "        c1, c2 = np.random.choice(subset, 2, replace=False)\n",
    "        aux = row[c1]\n",
    "        row[c1] = row[c2]\n",
    "        row[c2] = aux\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b163055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_set_empty_column(row, subset, p):\n",
    "    if np.random.rand() < p:\n",
    "        c = np.random.choice(subset)\n",
    "        row[c] = ''\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed945cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328f722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd6b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_db2 = []\n",
    "\n",
    "for _,row in db2.iterrows():\n",
    "    if ROW_COL_MISSING_OR_SWAPPED == 'SWAP':\n",
    "        row = _random_swap_columns(row,\n",
    "                                   subset=list(d_name_map_c1c2.values()), # ['name′', 'address′', 'job′']\n",
    "                                   p=P_ROW_COL_MISSING_OR_SWAPPED)\n",
    "    elif ROW_COL_MISSING_OR_SWAPPED == 'MISSING':\n",
    "        row = _random_set_empty_column(row,\n",
    "                                       subset=list(d_name_map_c1c2.values()), #  ['name′', 'address′', 'job′']\n",
    "                                       p=P_ROW_COL_MISSING_OR_SWAPPED)\n",
    "    elif ROW_COL_MISSING_OR_SWAPPED is None:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    rows_db2.append(row)\n",
    "    \n",
    "db2 = pd.DataFrame(rows_db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e05a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad25fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efefb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f88b512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amanda Arroyo</td>\n",
       "      <td>0751 Samantha Walk Apt. 650\\nChenfort, SC 04033</td>\n",
       "      <td>Herbalist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Victoria Brown</td>\n",
       "      <td>787 Alexander Road\\nPort Leslieborough, VA 53325</td>\n",
       "      <td>Outdoor activities/education manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amy Henry</td>\n",
       "      <td>706 Sarah Lakes Apt. 421\\nSouth Jeremy, AR 49313</td>\n",
       "      <td>Chemist, analytical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christopher Curtis</td>\n",
       "      <td>7274 Bird Canyon Suite 720\\nValentinechester, SC 19114</td>\n",
       "      <td>Geneticist, molecular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kevin Vargas</td>\n",
       "      <td>1310 Anderson Fork Apt. 598\\nBrandonbury, WV 31931</td>\n",
       "      <td>Structural engineer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                                 address  \\\n",
       "0       Amanda Arroyo         0751 Samantha Walk Apt. 650\\nChenfort, SC 04033   \n",
       "1      Victoria Brown        787 Alexander Road\\nPort Leslieborough, VA 53325   \n",
       "2           Amy Henry        706 Sarah Lakes Apt. 421\\nSouth Jeremy, AR 49313   \n",
       "3  Christopher Curtis  7274 Bird Canyon Suite 720\\nValentinechester, SC 19114   \n",
       "4        Kevin Vargas      1310 Anderson Fork Apt. 598\\nBrandonbury, WV 31931   \n",
       "\n",
       "                                    job  \n",
       "0                             Herbalist  \n",
       "1  Outdoor activities/education manager  \n",
       "2                   Chemist, analytical  \n",
       "3                 Geneticist, molecular  \n",
       "4                   Structural engineer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('max.colwidth', None):\n",
    "    display(db1[['name', 'address', 'job']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32860a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ed1aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name′</th>\n",
       "      <th>address′</th>\n",
       "      <th>job′</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arroyo</td>\n",
       "      <td>0s51 Chenfg4t</td>\n",
       "      <td>Herbulist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Victoria</td>\n",
       "      <td>5r325 i87</td>\n",
       "      <td>manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amy</td>\n",
       "      <td>Joremy 707</td>\n",
       "      <td>analytical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chrictopher</td>\n",
       "      <td>SCcCanyon</td>\n",
       "      <td>Ge9eti9est</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kevin</td>\n",
       "      <td>kV 1310</td>\n",
       "      <td>engineer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name′       address′        job′\n",
       "0       Arroyo  0s51 Chenfg4t   Herbulist\n",
       "1     Victoria      5r325 i87     manager\n",
       "2          Amy     Joremy 707  analytical\n",
       "3  Chrictopher      SCcCanyon  Ge9eti9est\n",
       "4        Kevin        kV 1310    engineer"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2[d_name_map_c1c2.values()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32181829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing rows + shuffle\n",
    "\n",
    "db2 = db2.sample(frac=FRAC_KEPT_ROWS_DB2, random_state=288).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7336859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db2.to_csv('db2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e389c12",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056e563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/02 14:12:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create PySpark SparkSession\n",
    "\n",
    "# .master('local[*]'): Set local computations\n",
    "# .config(\"spark.driver.memory\", \"15g\"): Set to avoid Java heap OOM\n",
    "# .config(\"spark.sql.shuffle.partitions\", \"2000\"): Set 2k instead of default 200 to avoid big partitions\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .appName('record-linkage') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cacdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa8bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 14:13:15 WARN TaskSetManager: Stage 0 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- residence: string (nullable = true)\n",
      " |-- current_location: string (nullable = true)\n",
      " |-- blood_group: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- mail: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1 = spark.createDataFrame(db1).repartition(200)\n",
    "print(sdf1.rdd.getNumPartitions())\n",
    "sdf1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4dbb77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = db2.rename(columns={\n",
    "    'job′': 'job',\n",
    "    'address′': 'address',\n",
    "    'name′': 'name'\n",
    "})\n",
    "db2['index'] = db2['index'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3964b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 14:13:24 WARN TaskSetManager: Stage 1 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "root\n",
      " |-- index: integer (nullable = false)\n",
      " |-- job: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "# Create User defined Custom Schema using StructType\n",
    "db2_schema = StructType([\n",
    "    StructField(\"index\", IntegerType(), False),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "sdf2 = spark.createDataFrame(db2[['index','job','address','name']], schema=db2_schema).repartition(200)\n",
    "print(sdf2.rdd.getNumPartitions())\n",
    "sdf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "859ba462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, NGram\n",
    "\n",
    "def char_tokenize(txt):\n",
    "    return [c for c in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95330af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_char_level_tfidf_pyspark(spark_df, col, ngram_n=3):\n",
    "    udf_tokenize = udf(char_tokenize, ArrayType(StringType(), False))\n",
    "    \n",
    "    ngram = NGram(n=ngram_n)\n",
    "    spark_df = spark_df.withColumn(f\"{col}_tok\", udf_tokenize(f\"{col}\"))\n",
    "\n",
    "    ngram.setInputCol(f\"{col}_tok\")\n",
    "    ngram.setOutputCol(f\"{col}_tok_ngrams\")\n",
    "\n",
    "    spark_df = ngram.transform(spark_df)\n",
    "\n",
    "    count_vec = CountVectorizer(inputCol=f\"{col}_tok_ngrams\", outputCol=f\"{col}_tf_tok_ngrams\")\n",
    "    count_vec = count_vec.fit(spark_df)\n",
    "    spark_df = count_vec.transform(spark_df)\n",
    "    \n",
    "    idf = IDF(inputCol=f\"{col}_tf_tok_ngrams\", outputCol=f\"{col}_tfidf_tok_ngrams\")\n",
    "    idf = idf.fit(spark_df)\n",
    "    spark_df = idf.transform(spark_df)\n",
    "    \n",
    "    return {\n",
    "        'count_vec': count_vec,\n",
    "        'idf': idf,\n",
    "        'spark_df': spark_df,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a7cbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_w_char_level_tfidf_pyspark(spark_df, col, count_vec, idf, ngram_n=3):\n",
    "    udf_tokenize = udf(char_tokenize, ArrayType(StringType(), False))\n",
    "    \n",
    "    ngram = NGram(n=ngram_n)\n",
    "    spark_df = spark_df.withColumn(f\"{col}_tok\", udf_tokenize(f\"{col}\"))\n",
    "\n",
    "    ngram.setInputCol(f\"{col}_tok\")\n",
    "    ngram.setOutputCol(f\"{col}_tok_ngrams\")\n",
    "    spark_df = ngram.transform(spark_df)\n",
    "    \n",
    "    count_vec.setInputCol(f\"{col}_tok_ngrams\")\n",
    "    count_vec.setOutputCol(f\"{col}_tf_tok_ngrams\")\n",
    "    spark_df = count_vec.transform(spark_df)\n",
    "    \n",
    "    idf.setInputCol(f\"{col}_tf_tok_ngrams\")\n",
    "    idf.setOutputCol(f\"{col}_tfidf_tok_ngrams\")\n",
    "    spark_df = idf.transform(spark_df)\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "\n",
    "# def vstack_pyspark_cols(spark_df1, col1, spark_df2, col2):\n",
    "#     return spark_df1.withColumnRenamed(col1, 'col').select(['col']).union(spark_df2.withColumnRenamed(col2, 'col').select(['col']))\n",
    "\n",
    "\n",
    "def vstack_pyspark_list_cols(spark_df1, list_cols1, spark_df2, list_cols2):\n",
    "    list_cols = []\n",
    "    for col in list_cols1:\n",
    "        list_cols.append(spark_df1.select(col).withColumnRenamed(col, 'col'))\n",
    "    for col in list_cols2:\n",
    "        list_cols.append(spark_df2.select(col).withColumnRenamed(col, 'col'))\n",
    "        \n",
    "    spark_df_aux = list_cols[0]\n",
    "    for spark_df_col in list_cols[1:]:\n",
    "        spark_df_aux = spark_df_aux.union(spark_df_col)\n",
    "        \n",
    "    return spark_df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "815f041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_out = vstack_pyspark_list_cols(spark_df1=sdf1, list_cols1=['job', 'address', 'name'], spark_df2=sdf2, list_cols2=['job', 'address', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d44b1e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 14:13:26 WARN TaskSetManager: Stage 2 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:13:29 WARN TaskSetManager: Stage 3 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:13:32 WARN TaskSetManager: Stage 4 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:13:34 WARN TaskSetManager: Stage 5 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:13:35 WARN TaskSetManager: Stage 6 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:13:35 WARN TaskSetManager: Stage 7 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:14:58 WARN TaskSetManager: Stage 24 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:15:01 WARN TaskSetManager: Stage 25 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:15:04 WARN TaskSetManager: Stage 26 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:15:06 WARN TaskSetManager: Stage 27 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:15:07 WARN TaskSetManager: Stage 28 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:15:07 WARN TaskSetManager: Stage 29 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 14:16:31 WARN DAGScheduler: Broadcasting large task binary with size 1000.8 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "d_ret_fit_tfidf = fit_char_level_tfidf_pyspark(sdf_out, 'col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ace43456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, job: string, address: string, name: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "def3b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 15:04:54 WARN TaskSetManager: Stage 50 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf2 = transform_w_char_level_tfidf_pyspark(\n",
    "    spark_df=sdf2,\n",
    "    col='job',\n",
    "    count_vec=d_ret_fit_tfidf['count_vec'],\n",
    "    idf=d_ret_fit_tfidf['idf'],\n",
    "    ngram_n=3\n",
    ")\n",
    "sdf2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ba101ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Unnamed: 0: bigint, job: string, company: string, ssn: string, residence: string, current_location: string, blood_group: string, website: string, username: string, name: string, sex: string, address: string, mail: string, birthdate: string, job_tok: array<string>, job_tok_ngrams: array<string>, job_tf_tok_ngrams: vector, job_tfidf_tok_ngrams: vector]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b398fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1 = transform_w_char_level_tfidf_pyspark(\n",
    "    spark_df=sdf1,\n",
    "    col='job',\n",
    "    count_vec=d_ret_fit_tfidf['count_vec'],\n",
    "    idf=d_ret_fit_tfidf['idf'],\n",
    "    ngram_n=3\n",
    ")\n",
    "sdf1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21cef2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Unnamed: 0: bigint, job: string, company: string, ssn: string, residence: string, current_location: string, blood_group: string, website: string, username: string, name: string, sex: string, address: string, mail: string, birthdate: string, job_tok: array<string>, job_tok_ngrams: array<string>, job_tf_tok_ngrams: vector, job_tfidf_tok_ngrams: vector]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8267b7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 15:15:43 WARN TaskSetManager: Stage 62 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine similarity of two TF-IDF vectors can also be calculated as the dot product of two L2 normalized TF-IDF vectors\n",
    "# https://stackoverflow.com/a/46764347/7928119\n",
    "from pyspark.ml.feature import Normalizer\n",
    "normalizer = Normalizer(inputCol=\"job_tfidf_tok_ngrams\", outputCol=\"job_tfidf_tok_ngrams_l2norm\")\n",
    "sdf1 = normalizer.transform(sdf1)\n",
    "sdf1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ad9419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 15:19:57 WARN TaskSetManager: Stage 71 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf2 = normalizer.transform(sdf2)\n",
    "sdf2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1289d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b270eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 15:54:45 WARN TaskSetManager: Stage 92 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 92:>                                                         (0 + 4) / 4]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "# Column of sparse vectors to sparse mattrix\n",
    "# https://stackoverflow.com/a/47675305/7928119\n",
    "v1 = sdf1.select(\"job_tfidf_tok_ngrams_l2norm\").rdd.map(lambda row: row.job_tfidf_tok_ngrams_l2norm)\n",
    "m1 = RowMatrix(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67af4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6c8744dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 16:35:07 WARN TaskSetManager: Stage 106 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/05/02 16:35:09 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/05/02 16:35:09 ERROR Executor: Exception in task 0.0 in stage 108.0 (TID 5992)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_38517/3345332766.py\", line 3, in <lambda>\n",
      "TypeError: __init__() missing 1 required positional argument: 'vector'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/02 16:35:09 WARN TaskSetManager: Lost task 0.0 in stage 108.0 (TID 5992) (7a13ec49fbba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_38517/3345332766.py\", line 3, in <lambda>\n",
      "TypeError: __init__() missing 1 required positional argument: 'vector'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/05/02 16:35:09 ERROR TaskSetManager: Task 0 in stage 108.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 108.0 failed 1 times, most recent failure: Lost task 0.0 in stage 108.0 (TID 5992) (7a13ec49fbba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_38517/3345332766.py\", line 3, in <lambda>\nTypeError: __init__() missing 1 required positional argument: 'vector'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_38517/3345332766.py\", line 3, in <lambda>\nTypeError: __init__() missing 1 required positional argument: 'vector'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38517/3345332766.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedRow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexedRowMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job_tfidf_tok_ngrams_l2norm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndexedRowMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexedRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoBlockMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rows, numRows, numCols)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;31m# both be easily serialized.  We will convert back to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# IndexedRows on the Scala side.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             java_matrix = callMLlibFunc(\"createIndexedRowMatrix\", rows.toDF(),\n\u001b[0m\u001b[1;32m    601\u001b[0m                                         int(numRows), int(numCols))\n\u001b[1;32m    602\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \"\"\"\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \"\"\"\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \"\"\"\n\u001b[0;32m-> 1588\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 108.0 failed 1 times, most recent failure: Lost task 0.0 in stage 108.0 (TID 5992) (7a13ec49fbba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_38517/3345332766.py\", line 3, in <lambda>\nTypeError: __init__() missing 1 required positional argument: 'vector'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_38517/3345332766.py\", line 3, in <lambda>\nTypeError: __init__() missing 1 required positional argument: 'vector'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "ac = sdf1.select(\"job_tfidf_tok_ngrams_l2norm\")\n",
    "mata = IndexedRowMatrix(ac.rdd.map(lambda row: IndexedRow(row)))\n",
    "ma = mata.toBlockMatrix(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ac = offer_row.select('a')\n",
    "bc = offer_row.select('a')\n",
    "mata = IndexedRowMatrix(ac.rdd.map(lambda row: IndexedRow(*row)))\n",
    "matb = IndexedRowMatrix(bc.rdd.map(lambda row: IndexedRow(*row)))\n",
    "\n",
    "ma = mata.toBlockMatrix(100,100)\n",
    "mb = matb.toBlockMatrix(100,100)\n",
    "\n",
    "ans = ma.multiply(mb.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bdec69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 15:59:09 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/05/02 15:59:09 ERROR Executor: Exception in task 2.0 in stage 96.0 (TID 5970)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 67, in _convert_to_vector\n",
      "    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\n",
      "TypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/02 15:59:09 WARN TaskSetManager: Lost task 2.0 in stage 96.0 (TID 5970) (7a13ec49fbba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 67, in _convert_to_vector\n",
      "    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\n",
      "TypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/05/02 15:59:09 ERROR TaskSetManager: Task 2 in stage 96.0 failed 1 times; aborting job\n",
      "22/05/02 15:59:09 WARN TaskSetManager: Lost task 1.0 in stage 96.0 (TID 5969) (7a13ec49fbba executor driver): TaskKilled (Stage cancelled)\n",
      "22/05/02 15:59:09 WARN TaskSetManager: Lost task 0.0 in stage 96.0 (TID 5968) (7a13ec49fbba executor driver): TaskKilled (Stage cancelled)\n",
      "22/05/02 15:59:09 WARN TaskSetManager: Lost task 3.0 in stage 96.0 (TID 5971) (7a13ec49fbba executor driver): TaskKilled (Stage cancelled)\n",
      "22/05/02 15:59:09 WARN TaskSetManager: Lost task 4.0 in stage 96.0 (TID 5972) (7a13ec49fbba executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o827.numRows.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 96.0 failed 1 times, most recent failure: Lost task 2.0 in stage 96.0 (TID 5970) (7a13ec49fbba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 67, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numRows(RowMatrix.scala:76)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 67, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38517/3550548810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36mnumRows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_matrix_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numRows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o827.numRows.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 96.0 failed 1 times, most recent failure: Lost task 2.0 in stage 96.0 (TID 5970) (7a13ec49fbba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 67, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numRows(RowMatrix.scala:76)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 67, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "m1.numRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a851f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf1.select('job_tfidf_tok_ngrams_l2norm').show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "828c49ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 15:35:34 WARN TaskSetManager: Stage 78 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "v2 = sdf2.select(\"job_tfidf_tok_ngrams_l2norm\").rdd.map(lambda row: row.features)\n",
    "m2 = RowMatrix(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea79f3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 15:38:19 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/05/02 15:38:20 ERROR Executor: Exception in task 2.0 in stage 82.0 (TID 5941)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1573, in __getattr__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: 'features' is not in list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_38517/1488069231.py\", line 4, in <lambda>\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1578, in __getattr__\n",
      "    raise AttributeError(item)\n",
      "AttributeError: features\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/02 15:38:20 WARN TaskSetManager: Lost task 2.0 in stage 82.0 (TID 5941) (7a13ec49fbba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1573, in __getattr__\n",
      "    idx = self.__fields__.index(item)\n",
      "ValueError: 'features' is not in list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_38517/1488069231.py\", line 4, in <lambda>\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1578, in __getattr__\n",
      "    raise AttributeError(item)\n",
      "AttributeError: features\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/05/02 15:38:20 ERROR TaskSetManager: Task 2 in stage 82.0 failed 1 times; aborting job\n",
      "22/05/02 15:38:20 WARN TaskSetManager: Lost task 0.0 in stage 82.0 (TID 5939) (7a13ec49fbba executor driver): TaskKilled (Stage cancelled)\n",
      "22/05/02 15:38:20 WARN TaskSetManager: Lost task 3.0 in stage 82.0 (TID 5942) (7a13ec49fbba executor driver): TaskKilled (Stage cancelled)\n",
      "22/05/02 15:38:20 WARN TaskSetManager: Lost task 1.0 in stage 82.0 (TID 5940) (7a13ec49fbba executor driver): TaskKilled (Stage cancelled)\n",
      "22/05/02 15:38:20 WARN TaskSetManager: Lost task 4.0 in stage 82.0 (TID 5943) (7a13ec49fbba executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o619.numRows.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 82.0 failed 1 times, most recent failure: Lost task 2.0 in stage 82.0 (TID 5941) (7a13ec49fbba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1573, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'features' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_38517/1488069231.py\", line 4, in <lambda>\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1578, in __getattr__\n    raise AttributeError(item)\nAttributeError: features\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numRows(RowMatrix.scala:76)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1573, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'features' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_38517/1488069231.py\", line 4, in <lambda>\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1578, in __getattr__\n    raise AttributeError(item)\nAttributeError: features\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38517/3550548810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36mnumRows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_matrix_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numRows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o619.numRows.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 82.0 failed 1 times, most recent failure: Lost task 2.0 in stage 82.0 (TID 5941) (7a13ec49fbba executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1573, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'features' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_38517/1488069231.py\", line 4, in <lambda>\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1578, in __getattr__\n    raise AttributeError(item)\nAttributeError: features\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numRows(RowMatrix.scala:76)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1573, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'features' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_38517/1488069231.py\", line 4, in <lambda>\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1578, in __getattr__\n    raise AttributeError(item)\nAttributeError: features\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1889)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "m1.numRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4099e3a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RowMatrix' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38517/242251583.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RowMatrix' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "M = m1.multiply(m2.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0957367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "630b4746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 15:01:34 WARN TaskSetManager: Stage 38 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5db4d1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': CountVectorizerModel: uid=CountVectorizer_9fa2f37b5f18, vocabularySize=110028,\n",
       " 'idf': IDFModel: uid=IDF_7354e917a504, numDocs=4500000, numFeatures=110028,\n",
       " 'spark_df': DataFrame[col: string, col_tok: array<string>, col_tok_ngrams: array<string>, col_tf_tok_ngrams: vector, col_tfidf_tok_ngrams: vector]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ret_fit_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79faf473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/02 13:26:34 WARN TaskSetManager: Stage 2 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_aux.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f63408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7930c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326ab1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_db1\n",
    "cols_db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d84c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_all = vstack_pyspark_cols(sdf1, 'job', sdf2, 'job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "314e7028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/28 15:51:28 WARN TaskSetManager: Stage 18 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/28 15:51:31 WARN TaskSetManager: Stage 19 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/28 15:51:49 WARN TaskSetManager: Stage 28 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/28 15:51:51 WARN TaskSetManager: Stage 29 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "d_ret_tfidf = fit_char_level_tfidf_pyspark(spark_df=sdf_all, col='col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "088f210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': CountVectorizerModel: uid=CountVectorizer_21f95c44fd33, vocabularySize=48436,\n",
       " 'idf': IDFModel: uid=IDF_ec338830e747, numDocs=1500000, numFeatures=48436,\n",
       " 'spark_df': DataFrame[col: string, col_tok: array<string>, col_tok_ngrams: array<string>, col_tf_tok_ngrams: vector, col_tfidf_tok_ngrams: vector]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ret_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fc80c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Transformer.transform of IDFModel: uid=IDF_ec338830e747, numDocs=1500000, numFeatures=48436>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ret_tfidf['idf'].transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "854714bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/28 13:48:43 WARN TaskSetManager: Stage 20 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Unnamed: 0=125372, job='Pharmacologist', company='Herrera PLC', ssn='693-58-1880', residence='89503 Kathy Turnpike Suite 249\\nNorth Ronald, WI 83639', current_location=\"(Decimal('72.3116095'), Decimal('-42.207814'))\", blood_group='O-', website=\"['https://www.arnold-trujillo.com/', 'http://www.castillo-lloyd.com/', 'https://hampton.net/', 'https://marshall.com/']\", username='aaronramirez', name='David Thompson', sex='M', address='949 Rice Rapid Apt. 119\\nWalkerfurt, WI 34835', mail='xbrandt@gmail.com', birthdate='2004-08-16', job_tok=['P', 'h', 'a', 'r', 'm', 'a', 'c', 'o', 'l', 'o', 'g', 'i', 's', 't'], job_tok_ngrams=['P h a', 'h a r', 'a r m', 'r m a', 'm a c', 'a c o', 'c o l', 'o l o', 'l o g', 'o g i', 'g i s', 'i s t'], tf_job_tok_ngrams=SparseVector(1893, {0: 1.0, 29: 1.0, 32: 1.0, 33: 1.0, 35: 1.0, 131: 1.0, 323: 1.0, 414: 1.0, 617: 1.0, 753: 1.0, 904: 1.0, 1700: 1.0}), tfidf_job_tok_ngrams=SparseVector(1893, {0: 1.3194, 29: 2.5272, 32: 2.5465, 33: 2.5667, 35: 2.6073, 131: 3.524, 323: 4.1604, 414: 4.3884, 617: 4.8607, 753: 5.0851, 904: 5.3651, 1700: 6.4708})),\n",
       " Row(Unnamed: 0=179221, job='Associate Professor', company='Lee-Olson', ssn='643-37-8682', residence='08094 Suarez Center Apt. 410\\nLisaburgh, UT 76403', current_location=\"(Decimal('-63.6904745'), Decimal('17.944387'))\", blood_group='AB-', website=\"['http://davis.com/', 'http://www.wood.org/']\", username='adamssally', name='Joseph Murphy', sex='M', address='5256 Eric Roads Suite 484\\nStaffordton, CT 29815', mail='carla00@hotmail.com', birthdate='1961-04-16', job_tok=['A', 's', 's', 'o', 'c', 'i', 'a', 't', 'e', ' ', 'P', 'r', 'o', 'f', 'e', 's', 's', 'o', 'r'], job_tok_ngrams=['A s s', 's s o', 's o c', 'o c i', 'c i a', 'i a t', 'a t e', 't e  ', 'e   P', '  P r', 'P r o', 'r o f', 'o f e', 'f e s', 'e s s', 's s o', 's o r'], tf_job_tok_ngrams=SparseVector(1893, {58: 1.0, 133: 1.0, 136: 1.0, 221: 1.0, 337: 1.0, 439: 1.0, 515: 2.0, 520: 1.0, 630: 1.0, 631: 1.0, 632: 1.0, 744: 1.0, 824: 1.0, 1745: 1.0, 1749: 1.0, 1751: 1.0}), tfidf_job_tok_ngrams=SparseVector(1893, {58: 2.9678, 133: 3.562, 136: 3.5661, 221: 3.8955, 337: 4.2567, 439: 4.5079, 515: 9.7055, 520: 4.6771, 630: 4.8667, 631: 4.8667, 632: 4.8667, 744: 5.0811, 824: 5.3502, 1745: 6.476, 1749: 6.476, 1751: 6.476}))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edc991d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDF_80b77a8fa561"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d181039",
   "metadata": {},
   "outputs": [],
   "source": [
    "(inputCol=\"words\", outputCol=\"features\")\n",
    "      .fit(original_df)\n",
    "      .transform(original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ffc79e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'inputCol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30946/3976053424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHashingTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"job_tok_ngrams\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf_job_tok_ngrams\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'inputCol'"
     ]
    }
   ],
   "source": [
    "HashingTF(inputCol=\"job_tok_ngrams\", outputCol=\"tf_job_tok_ngrams\").transform(sdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94470521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4024b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6ce28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "02b4f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/28 11:42:52 WARN TaskSetManager: Stage 10 contains a task of very large size (4951 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(index=222252, job='and', address='88787 M4rtibez', name='Lisb'),\n",
       " Row(index=94171, job='surveyob', address='Logarland 15323', name='Williams'),\n",
       " Row(index=147772, job='Surveyor', address='Ville Mloneybe9g', name='Eriza'),\n",
       " Row(index=465693, job='Libracian', address='Vilyages Jogn', name='Perkins'),\n",
       " Row(index=318978, job='Airlinv', address='5c287 Shoal', name='Veronica'),\n",
       " Row(index=246490, job='Fisheries', address='aa9senberp UT', name='Campbell'),\n",
       " Row(index=217914, job='assiss7nt', address='Park Ronals', name='Bass'),\n",
       " Row(index=544156, job='safety', address='Loc0 38gm8', name='Charles'),\n",
       " Row(index=76224, job='aherapist', address='ewive Chrishopher', name='Edwin'),\n",
       " Row(index=377407, job='of', address='Dustiaborough West', name='Mille8')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89a47f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e0b2c382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amanda Arroyo</td>\n",
       "      <td>Herbalist</td>\n",
       "      <td>Norton-Castillo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Victoria Brown</td>\n",
       "      <td>Outdoor activities/education manager</td>\n",
       "      <td>Bowman-Jensen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amy Henry</td>\n",
       "      <td>Chemist, analytical</td>\n",
       "      <td>Wilkerson, Guerrero and Mason</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                                   job  \\\n",
       "0   Amanda Arroyo                             Herbalist   \n",
       "1  Victoria Brown  Outdoor activities/education manager   \n",
       "2       Amy Henry                   Chemist, analytical   \n",
       "\n",
       "                         company  \n",
       "0                Norton-Castillo  \n",
       "1                  Bowman-Jensen  \n",
       "2  Wilkerson, Guerrero and Mason  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1[['name', 'job', 'company']].iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b78d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5c6e738d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "42d1f015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6ea29034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/28 13:17:25 WARN TaskSetManager: Stage 21 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 21:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7223f372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGram_ca03f062ea9b"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00c119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b42ffcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGram_57965cb425e8"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d3b2c3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "622404cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/28 13:20:20 WARN TaskSetManager: Stage 22 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Unnamed: 0=125372, job='Pharmacologist', company='Herrera PLC', ssn='693-58-1880', residence='89503 Kathy Turnpike Suite 249\\nNorth Ronald, WI 83639', current_location=\"(Decimal('72.3116095'), Decimal('-42.207814'))\", blood_group='O-', website=\"['https://www.arnold-trujillo.com/', 'http://www.castillo-lloyd.com/', 'https://hampton.net/', 'https://marshall.com/']\", username='aaronramirez', name='David Thompson', sex='M', address='949 Rice Rapid Apt. 119\\nWalkerfurt, WI 34835', mail='xbrandt@gmail.com', birthdate='2004-08-16', job_tok=['P', 'h', 'a', 'r', 'm', 'a', 'c', 'o', 'l', 'o', 'g', 'i', 's', 't'], job_tok_ngrams=['P h a', 'h a r', 'a r m', 'r m a', 'm a c', 'a c o', 'c o l', 'o l o', 'l o g', 'o g i', 'g i s', 'i s t']),\n",
       " Row(Unnamed: 0=179221, job='Associate Professor', company='Lee-Olson', ssn='643-37-8682', residence='08094 Suarez Center Apt. 410\\nLisaburgh, UT 76403', current_location=\"(Decimal('-63.6904745'), Decimal('17.944387'))\", blood_group='AB-', website=\"['http://davis.com/', 'http://www.wood.org/']\", username='adamssally', name='Joseph Murphy', sex='M', address='5256 Eric Roads Suite 484\\nStaffordton, CT 29815', mail='carla00@hotmail.com', birthdate='1961-04-16', job_tok=['A', 's', 's', 'o', 'c', 'i', 'a', 't', 'e', ' ', 'P', 'r', 'o', 'f', 'e', 's', 's', 'o', 'r'], job_tok_ngrams=['A s s', 's s o', 's o c', 'o c i', 'c i a', 'i a t', 'a t e', 't e  ', 'e   P', '  P r', 'P r o', 'r o f', 'o f e', 'f e s', 'e s s', 's s o', 's o r'])]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1961657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2c5ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e22e1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text='aaa bbbb cc', words=['aaa', 'bbbb', 'cc'])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"aaa bbbb cc\",)], [\"text\"])\n",
    "tokenizer = Tokenizer(outputCol=\"words\")\n",
    "tokenizer.setInputCol(\"text\")\n",
    "tokenizer.transform(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a310ece",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field name′: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26027/942142971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m--> 674\u001b[0;31m                 data, schema, samplingRatio, verifySchema)\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1110\u001b[0m                                                   name=new_name(f.name)))\n\u001b[0;32m-> 1111\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1110\u001b[0m                                                   name=new_name(f.name)))\n\u001b[0;32m-> 1111\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: field name′: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "sdf2 = spark.createDataFrame(db2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f509b1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4a84d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Unnamed: 0=125372, job='Pharmacologist', company='Herrera PLC', ssn='693-58-1880', residence='89503 Kathy Turnpike Suite 249\\nNorth Ronald, WI 83639', current_location=\"(Decimal('72.3116095'), Decimal('-42.207814'))\", blood_group='O-', website=\"['https://www.arnold-trujillo.com/', 'http://www.castillo-lloyd.com/', 'https://hampton.net/', 'https://marshall.com/']\", username='aaronramirez', name='David Thompson', sex='M', address='949 Rice Rapid Apt. 119\\nWalkerfurt, WI 34835', mail='xbrandt@gmail.com', birthdate='2004-08-16'),\n",
       " Row(Unnamed: 0=179221, job='Associate Professor', company='Lee-Olson', ssn='643-37-8682', residence='08094 Suarez Center Apt. 410\\nLisaburgh, UT 76403', current_location=\"(Decimal('-63.6904745'), Decimal('17.944387'))\", blood_group='AB-', website=\"['http://davis.com/', 'http://www.wood.org/']\", username='adamssally', name='Joseph Murphy', sex='M', address='5256 Eric Roads Suite 484\\nStaffordton, CT 29815', mail='carla00@hotmail.com', birthdate='1961-04-16'),\n",
       " Row(Unnamed: 0=189083, job='Designer, multimedia', company='Johnson, Melendez and Martinez', ssn='032-47-7287', residence='PSC 3073, Box 8732\\nAPO AA 54962', current_location=\"(Decimal('7.4636255'), Decimal('-46.460889'))\", blood_group='AB+', website=\"['https://www.thompson.info/', 'http://combs.com/', 'https://garza.net/', 'https://www.diaz.com/']\", username='crivera', name='Andrew Proctor', sex='M', address='7663 Patrick Islands Suite 956\\nPort Patrick, NJ 02112', mail='owensabigail@gmail.com', birthdate='1986-02-28'),\n",
       " Row(Unnamed: 0=11697, job='Seismic interpreter', company='Soto-Lee', ssn='570-21-7545', residence='7084 Andrea Stravenue\\nNew Emily, NC 12110', current_location=\"(Decimal('-55.4687965'), Decimal('-152.556338'))\", blood_group='AB-', website=\"['http://www.ford.com/', 'https://www.white-cole.com/', 'http://www.cobb-harris.com/']\", username='brittanywilliams', name='Allen Pierce', sex='M', address='49761 Smith Manor Suite 079\\nDillonton, ID 92221', mail='gary23@hotmail.com', birthdate='1992-05-20'),\n",
       " Row(Unnamed: 0=175586, job='Psychologist, sport and exercise', company='Wells and Sons', ssn='708-13-8745', residence='29973 Dwayne Passage Apt. 387\\nJamesland, HI 90779', current_location=\"(Decimal('-18.8101535'), Decimal('-145.550153'))\", blood_group='AB+', website=\"['http://www.waters-garcia.com/', 'https://larson.biz/']\", username='rreese', name='Jessica Molina', sex='F', address='058 Jeremy Shore Apt. 540\\nNorth Karenland, KY 34084', mail='mejiatabitha@gmail.com', birthdate='1959-12-04'),\n",
       " Row(Unnamed: 0=243289, job='Administrator, Civil Service', company='Hodge PLC', ssn='001-84-6057', residence='80306 Cole Canyon Suite 044\\nWalkerfurt, RI 32960', current_location=\"(Decimal('-4.364945'), Decimal('-104.677984'))\", blood_group='O+', website=\"['http://www.bailey.net/', 'http://dunn.com/', 'http://www.davis.com/', 'http://weaver-jarvis.org/']\", username='emily10', name='Jennifer Horton', sex='F', address='5544 Adam Crossing\\nCruztown, RI 14630', mail='nyoung@yahoo.com', birthdate='1924-04-06'),\n",
       " Row(Unnamed: 0=10541, job='Therapist, art', company='Foster, Campbell and Smith', ssn='492-78-2731', residence='USCGC Smith\\nFPO AE 17749', current_location=\"(Decimal('-28.655925'), Decimal('-114.845487'))\", blood_group='AB+', website=\"['https://washington-burke.info/']\", username='joelgarcia', name='Nicole Singleton', sex='F', address='29811 Laura Path Suite 516\\nWest Rhondastad, OH 68223', mail='serranoethan@hotmail.com', birthdate='1949-08-07'),\n",
       " Row(Unnamed: 0=151433, job='Horticulturist, amenity', company='Henry-Walker', ssn='042-59-1483', residence='670 Parsons Freeway\\nThompsonchester, TN 39501', current_location=\"(Decimal('79.8048075'), Decimal('-105.572495'))\", blood_group='AB+', website=\"['https://foster.com/', 'http://tyler.com/', 'http://www.holland.com/', 'https://sanchez.com/']\", username='karen88', name='Rebekah Kirby', sex='F', address='PSC 9944, Box 9655\\nAPO AP 66827', mail='reynoldsdaniel@hotmail.com', birthdate='2015-02-04'),\n",
       " Row(Unnamed: 0=18049, job='Data scientist', company='Harper Group', ssn='122-20-5534', residence='89131 Catherine Drive Apt. 306\\nSouth Brycefort, NV 89327', current_location=\"(Decimal('-17.685340'), Decimal('-95.893011'))\", blood_group='B+', website=\"['https://livingston-lee.com/', 'https://www.kirk-jackson.com/']\", username='joneswendy', name='Emily Walker', sex='F', address='1453 Kaitlyn Knoll Suite 890\\nDonaldfort, DE 57632', mail='blackburnheather@hotmail.com', birthdate='2016-10-11'),\n",
       " Row(Unnamed: 0=218232, job='Waste management officer', company='Chavez-Mitchell', ssn='314-31-7148', residence='59997 Lisa Ferry Apt. 632\\nFowlermouth, LA 48824', current_location=\"(Decimal('-44.5962305'), Decimal('151.378775'))\", blood_group='B-', website=\"['https://smith-watson.org/', 'https://espinoza.biz/', 'http://woodard.com/', 'https://rodriguez-johnson.info/']\", username='greenbrian', name='Sarah Gutierrez', sex='F', address='3157 Colleen Hills\\nEast Alejandro, UT 62552', mail='obaldwin@yahoo.com', birthdate='1978-09-06')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf1.rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "039cd572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/28 10:54:26 WARN TaskSetManager: Stage 1 contains a task of very large size (82296 KiB). The maximum recommended task size is 1000 KiB.\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24999/49932049.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sdf1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e67128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3101ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0a5bb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "db1 = pd.read_excel(dir_data / 'POS Data Melilla.xlsx').dropna(how='all', axis=1)\n",
    "db2 = pd.read_excel(dir_data / 'Nielsen data Melilla.xlsx', header=1).dropna(how='all', axis=1)\n",
    "\n",
    "db1_name = 'POS'\n",
    "db2_name = 'Nielsen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0970cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc1fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f7c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef752a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0ac0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toni: reduce dim\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(analyzer=TFIDF_ANALYZER,\n",
    "                        ngram_range=(TFIDF_NGRAM_LO, TFIDF_NGRAM_HI),\n",
    "                        max_df=TFIDF_MAX_DF,\n",
    "                        min_df=TFIDF_MIN_DF,\n",
    "                        max_features=TFIDF_MAX_FEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27006f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([\n",
    "    [str(x) for x in db1.to_numpy().flatten()],\n",
    "    [str(x) for x in db2.to_numpy().flatten()]\n",
    "])\n",
    "tfidf.fit(X)\n",
    "TFIDF_VOCAB = len(tfidf.vocabulary_)\n",
    "TFIDF_VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0637b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "729680d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "l_d_sim = []\n",
    "for c1 in db1.columns:\n",
    "    for c2 in db2.columns:\n",
    "        # Do not take into account NaN values in these columns\n",
    "        X1 = tfidf.transform([str(x) for x in db1[c1].values if pd.notna(x)]).astype(np.float32) # cast to np.float32 to avoid mem issues in cosine_sim mat\n",
    "        X2 = tfidf.transform([str(x) for x in db2[c2].values if pd.notna(x)]).astype(np.float32)\n",
    "        M = cosine_similarity(X1, X2)\n",
    "        max_sim_c1c2 = M.max(axis=1).mean()\n",
    "        max_sim_c2c1 = M.max(axis=0).mean()\n",
    "        max_sim_mean = np.mean([max_sim_c1c2, max_sim_c2c1])\n",
    "        l_d_sim.append({\n",
    "            'c1': c1,\n",
    "            'c2': c2,\n",
    "            'sim': max_sim_mean\n",
    "        })\n",
    "#         print(f'Similarity index between DB1({c1}) and DB2({c2}): {max_sim_c1c2}')\n",
    "df_sim = pd.DataFrame(l_d_sim).sort_values('sim', ascending=False)\n",
    "\n",
    "\n",
    "TO_MATCH = set(db1.columns).union(set(db2.columns))#[\"name'\", \"company'\", \"address'\"]\n",
    "matched = set()\n",
    "rows_kept = []\n",
    "\n",
    "for _, row in df_sim.iterrows():\n",
    "    c1 = row['c1']\n",
    "    c2 = row['c2']\n",
    "    if (c1 in matched or c2 in matched)\\\n",
    "    or (c1 not in TO_MATCH and c2 not in TO_MATCH):\n",
    "        continue\n",
    "    else:\n",
    "        matched = matched.union(set([c1, c2]))\n",
    "        rows_kept.append(row)\n",
    "\n",
    "\n",
    "df_c1c2 = pd.DataFrame(rows_kept)\n",
    "\n",
    "# d_c1c2 = df_c1c2.set_index('c1')['c2'].to_dict()\n",
    "# d_c1c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1900adad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        NaN\n",
       "1      374.0\n",
       "2        NaN\n",
       "3     1419.0\n",
       "4        NaN\n",
       "5       38.0\n",
       "6     6419.0\n",
       "7        NaN\n",
       "8     6889.0\n",
       "9        NaN\n",
       "10    7153.0\n",
       "11    7287.0\n",
       "12     744.0\n",
       "Name: RANK NACIONAL, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1['RANK NACIONAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e372fea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    NaN\n",
       "1                    NaN\n",
       "2                    NaN\n",
       "3                    NaN\n",
       "4                    NaN\n",
       "5                    NaN\n",
       "6                    NaN\n",
       "7                    NaN\n",
       "8                    NaN\n",
       "9                    NaN\n",
       "10    C.C.PARQUE MELILLA\n",
       "11                   NaN\n",
       "12                   NaN\n",
       "Name: CENTROCIAL, dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2['CENTROCIAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7c953b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IR Store_code</td>\n",
       "      <td>CODIGO</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>ZIP</td>\n",
       "      <td>COD_POSTAL</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>SUPERFN</td>\n",
       "      <td>SUPERFN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>MUNICIPIO</td>\n",
       "      <td>MUNICIPIO</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>CHAIN SF</td>\n",
       "      <td>CADENA</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>PROVINCIA</td>\n",
       "      <td>PROVINCIA</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>TIPO_NIELSEN_NEW</td>\n",
       "      <td>TIPO_NIELSEN_NEW</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>CCAA</td>\n",
       "      <td>COMUNIDAD_AUTONOMA</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>ROTULO</td>\n",
       "      <td>ROTULO</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>AGR.2019 (P_15)</td>\n",
       "      <td>POT_15</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>ADDRESS</td>\n",
       "      <td>DIRECCION</td>\n",
       "      <td>0.754297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>RANK NACIONAL</td>\n",
       "      <td>POT_8</td>\n",
       "      <td>0.295982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>CHANNEL P&amp;G</td>\n",
       "      <td>DIRECCION1</td>\n",
       "      <td>0.189351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>BANNER CONTROL</td>\n",
       "      <td>DIRECCION_COD</td>\n",
       "      <td>0.102067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>CANAL SF</td>\n",
       "      <td>ABRV</td>\n",
       "      <td>0.099444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>BANNER SF (COMPILADO)</td>\n",
       "      <td>MACROCADENAS_NOMBRE</td>\n",
       "      <td>0.078307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>CODE</td>\n",
       "      <td>NUMERO</td>\n",
       "      <td>0.062269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>NAME STORE (ID)</td>\n",
       "      <td>ORGANIZADO</td>\n",
       "      <td>0.054005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>COBERTURA FY2122</td>\n",
       "      <td>AREAN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        c1                   c2       sim\n",
       "0            IR Store_code               CODIGO  1.000000\n",
       "379                    ZIP           COD_POSTAL  1.000000\n",
       "535                SUPERFN              SUPERFN  1.000000\n",
       "494              MUNICIPIO            MUNICIPIO  1.000000\n",
       "150               CHAIN SF               CADENA  1.000000\n",
       "456              PROVINCIA            PROVINCIA  1.000000\n",
       "573       TIPO_NIELSEN_NEW     TIPO_NIELSEN_NEW  1.000000\n",
       "418                   CCAA   COMUNIDAD_AUTONOMA  1.000000\n",
       "188                 ROTULO               ROTULO  1.000000\n",
       "663        AGR.2019 (P_15)               POT_15  1.000000\n",
       "338                ADDRESS            DIRECCION  0.754297\n",
       "101          RANK NACIONAL                POT_8  0.295982\n",
       "599            CHANNEL P&G           DIRECCION1  0.189351\n",
       "230         BANNER CONTROL        DIRECCION_COD  0.102067\n",
       "41                CANAL SF                 ABRV  0.099444\n",
       "260  BANNER SF (COMPILADO)  MACROCADENAS_NOMBRE  0.078307\n",
       "117                   CODE               NUMERO  0.062269\n",
       "312        NAME STORE (ID)           ORGANIZADO  0.054005\n",
       "676       COBERTURA FY2122                AREAN  0.000000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c1c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "20885816",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Detected column matches with high matching score between:\n",
       " - DB1 (`POS`) and,\n",
       " - DB2 (`Nielsen`)\n",
       "### Showing column matches with score `> 0.5`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Column in POS</th>\n",
       "      <th>Column in Nielsen</th>\n",
       "      <th>match_score (column)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>IR Store_code</td>\n",
       "      <td>CODIGO</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ZIP</td>\n",
       "      <td>COD_POSTAL</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SUPERFN</td>\n",
       "      <td>SUPERFN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MUNICIPIO</td>\n",
       "      <td>MUNICIPIO</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CHAIN SF</td>\n",
       "      <td>CADENA</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PROVINCIA</td>\n",
       "      <td>PROVINCIA</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TIPO_NIELSEN_NEW</td>\n",
       "      <td>TIPO_NIELSEN_NEW</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CCAA</td>\n",
       "      <td>COMUNIDAD_AUTONOMA</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ROTULO</td>\n",
       "      <td>ROTULO</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AGR.2019 (P_15)</td>\n",
       "      <td>POT_15</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ADDRESS</td>\n",
       "      <td>DIRECCION</td>\n",
       "      <td>0.754297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import Markdown, HTML\n",
    "THR_SIM_COL = 0.5\n",
    "display(Markdown(f'''\n",
    "### Detected column matches with high matching score between:\n",
    " - DB1 (`{db1_name}`) and,\n",
    " - DB2 (`{db2_name}`)\n",
    "### Showing column matches with score `> {THR_SIM_COL}`\n",
    "'''))\n",
    "\n",
    "df_c1c2_filtered = df_c1c2.query(f'sim > {THR_SIM_COL}').rename(columns={\n",
    "    'c1': f'Column in {db1_name}',\n",
    "    'c2': f'Column in {db2_name}',\n",
    "    'sim': 'match_score (column)',\n",
    "})\n",
    "\n",
    "display(HTML(df_c1c2_filtered.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc7553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dd431e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IR Store_code</td>\n",
       "      <td>CODIGO</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>ZIP</td>\n",
       "      <td>COD_POSTAL</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>SUPERFN</td>\n",
       "      <td>SUPERFN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>MUNICIPIO</td>\n",
       "      <td>MUNICIPIO</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>CHAIN SF</td>\n",
       "      <td>CADENA</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>PROVINCIA</td>\n",
       "      <td>PROVINCIA</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>TIPO_NIELSEN_NEW</td>\n",
       "      <td>TIPO_NIELSEN_NEW</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>CCAA</td>\n",
       "      <td>COMUNIDAD_AUTONOMA</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>ROTULO</td>\n",
       "      <td>ROTULO</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>AGR.2019 (P_15)</td>\n",
       "      <td>POT_15</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>ADDRESS</td>\n",
       "      <td>DIRECCION</td>\n",
       "      <td>0.754297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>RANK NACIONAL</td>\n",
       "      <td>POT_8</td>\n",
       "      <td>0.295982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>CHANNEL P&amp;G</td>\n",
       "      <td>DIRECCION1</td>\n",
       "      <td>0.189351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>BANNER CONTROL</td>\n",
       "      <td>DIRECCION_COD</td>\n",
       "      <td>0.102067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>CANAL SF</td>\n",
       "      <td>ABRV</td>\n",
       "      <td>0.099444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>BANNER SF (COMPILADO)</td>\n",
       "      <td>MACROCADENAS_NOMBRE</td>\n",
       "      <td>0.078307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>CODE</td>\n",
       "      <td>NUMERO</td>\n",
       "      <td>0.062269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>NAME STORE (ID)</td>\n",
       "      <td>ORGANIZADO</td>\n",
       "      <td>0.054005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>COBERTURA FY2122</td>\n",
       "      <td>AREAN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        c1                   c2       sim\n",
       "0            IR Store_code               CODIGO  1.000000\n",
       "379                    ZIP           COD_POSTAL  1.000000\n",
       "535                SUPERFN              SUPERFN  1.000000\n",
       "494              MUNICIPIO            MUNICIPIO  1.000000\n",
       "150               CHAIN SF               CADENA  1.000000\n",
       "456              PROVINCIA            PROVINCIA  1.000000\n",
       "573       TIPO_NIELSEN_NEW     TIPO_NIELSEN_NEW  1.000000\n",
       "418                   CCAA   COMUNIDAD_AUTONOMA  1.000000\n",
       "188                 ROTULO               ROTULO  1.000000\n",
       "663        AGR.2019 (P_15)               POT_15  1.000000\n",
       "338                ADDRESS            DIRECCION  0.754297\n",
       "101          RANK NACIONAL                POT_8  0.295982\n",
       "599            CHANNEL P&G           DIRECCION1  0.189351\n",
       "230         BANNER CONTROL        DIRECCION_COD  0.102067\n",
       "41                CANAL SF                 ABRV  0.099444\n",
       "260  BANNER SF (COMPILADO)  MACROCADENAS_NOMBRE  0.078307\n",
       "117                   CODE               NUMERO  0.062269\n",
       "312        NAME STORE (ID)           ORGANIZADO  0.054005\n",
       "676       COBERTURA FY2122                AREAN  0.000000"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c1c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2121c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name_map_c1c2 = df_c1c2_filtered[['Column in POS', 'Column in Nielsen']].set_index('Column in POS').to_dict()['Column in Nielsen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "43639c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IR Store_code</th>\n",
       "      <th>CANAL SF</th>\n",
       "      <th>RANK NACIONAL</th>\n",
       "      <th>CODE</th>\n",
       "      <th>CHAIN SF</th>\n",
       "      <th>ADDRESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5200016</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>MERCADONA, S.A.</td>\n",
       "      <td>CL DE LOS VELEZ, MARQUES SN CLCARLOS V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5200014</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>374.0</td>\n",
       "      <td>SUPER 1-5</td>\n",
       "      <td>GRUPO EROSKI</td>\n",
       "      <td>CL CUARTEL VALENZUELA SN  C.C.PARQUE MELILLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5200007</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>LIDL SUPERMERCADOS, S.A.U.</td>\n",
       "      <td>CL MAANAN BENAISA MIMUM SN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5200001</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>1419.0</td>\n",
       "      <td>RESTO SUPER 51-184</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>CL GENERAL POLAVIEJA 30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5200008</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>GM FOOD IBERICA</td>\n",
       "      <td>CL FERNANDEZ CUEVAS 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5200003</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>38.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>CL ALCALDE RAFAEL GINEL 12 EDIF. SAN LORENZO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5200009</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>6419.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>CL MARQUES DE MONTEMAR 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5200006</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>COVIRAN, S.C.A.</td>\n",
       "      <td>CL ALFONSO XIII SN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5200015</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>6889.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>CL GENERAL VILLALBA 48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5200004</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>COVIRAN, S.C.A.</td>\n",
       "      <td>CL LAS MARGARITAS SN NAVE 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5200011</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>7153.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>CL GARCIA CABRELLES 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5200012</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>7287.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>CL CABRERIZAS 17 PLANTA 7-PUERTA B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5200002</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>744.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>CL MADRID SN EDIF. ZURBARAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IR Store_code        CANAL SF RANK NACIONAL                CODE  \\\n",
       "0        5200016     CANAL NO SF           nan                 nan   \n",
       "1        5200014  SUPER NACIONAL         374.0           SUPER 1-5   \n",
       "2        5200007     CANAL NO SF           nan                 nan   \n",
       "3        5200001  SUPER REGIONAL        1419.0  RESTO SUPER 51-184   \n",
       "4        5200008     CANAL NO SF           nan                 nan   \n",
       "5        5200003  SUPER REGIONAL          38.0         RESTO SUPER   \n",
       "6        5200009  SUPER NACIONAL        6419.0         RESTO SUPER   \n",
       "7        5200006     CANAL NO SF           nan                 nan   \n",
       "8        5200015  SUPER NACIONAL        6889.0         RESTO SUPER   \n",
       "9        5200004     CANAL NO SF           nan                 nan   \n",
       "10       5200011  SUPER NACIONAL        7153.0         RESTO SUPER   \n",
       "11       5200012  SUPER NACIONAL        7287.0         RESTO SUPER   \n",
       "12       5200002  SUPER REGIONAL         744.0         RESTO SUPER   \n",
       "\n",
       "                      CHAIN SF                                        ADDRESS  \n",
       "0              MERCADONA, S.A.        CL DE LOS VELEZ, MARQUES SN CLCARLOS V   \n",
       "1                 GRUPO EROSKI   CL CUARTEL VALENZUELA SN  C.C.PARQUE MELILLA  \n",
       "2   LIDL SUPERMERCADOS, S.A.U.                   CL MAANAN BENAISA MIMUM SN    \n",
       "3         SUPERSOL SPAIN, S.L.                      CL GENERAL POLAVIEJA 30    \n",
       "4              GM FOOD IBERICA                        CL FERNANDEZ CUEVAS 1    \n",
       "5         SUPERSOL SPAIN, S.L.  CL ALCALDE RAFAEL GINEL 12 EDIF. SAN LORENZO   \n",
       "6                    DIA, S.A.                     CL MARQUES DE MONTEMAR 5    \n",
       "7              COVIRAN, S.C.A.                           CL ALFONSO XIII SN    \n",
       "8                    DIA, S.A.                       CL GENERAL VILLALBA 48    \n",
       "9              COVIRAN, S.C.A.                   CL LAS MARGARITAS SN NAVE 1   \n",
       "10                   DIA, S.A.                       CL GARCIA CABRELLES 20    \n",
       "11                   DIA, S.A.            CL CABRERIZAS 17 PLANTA 7-PUERTA B   \n",
       "12        SUPERSOL SPAIN, S.L.                   CL MADRID SN EDIF. ZURBARAN   "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1[['IR Store_code','CANAL SF','RANK NACIONAL','CODE','CHAIN SF','ADDRESS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f86269ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3dd90dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3c91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "102ec0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7a64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5bfd89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare same width dataframes with correspondences to extract row vectors\n",
    "db1_to_match = pd.concat([db1[c] for c in d_name_map_c1c2.keys()], axis=1, ignore_index=True)\n",
    "db2_to_match = pd.concat([db2[c] for c in d_name_map_c1c2.values()], axis=1, ignore_index=True)\n",
    "assert db1_to_match.shape[1] == db2_to_match.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "52832c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast all to str\n",
    "for db in [db1, db2]:\n",
    "    for c in db.columns:\n",
    "        db[c] = db[c].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758c1de",
   "metadata": {},
   "source": [
    "# Rows matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8793f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack, hstack\n",
    "def vectorize_df_ordered_cols(df):\n",
    "    vectorized_tfidf = np.vectorize(lambda x: tfidf.transform([x]))\n",
    "    db_mat = df.to_numpy()\n",
    "    X_db = vectorized_tfidf(db_mat)\n",
    "    X_db = [hstack(row) for row in X_db]\n",
    "    X_db = vstack(X_db)\n",
    "    return X_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7bca66a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change tick() to be the one to have the name, and not tack(): more descriptive\n",
    "class Clock():\n",
    "    def __init__(self):\n",
    "        self.d = {}\n",
    "        \n",
    "    def tick(self):\n",
    "        self.t = time()\n",
    "        \n",
    "    def tack(self, name):\n",
    "        self.d[name] = round(time() - self.t, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c4735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8f46b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "clock = Clock()\n",
    "\n",
    "clock.tick()\n",
    "X_db1 = vectorize_df_ordered_cols(db1[d_name_map_c1c2.keys()]).astype(np.float32)\n",
    "clock.tack('time_vectorize_db1')\n",
    "\n",
    "clock.tick()\n",
    "X_db2 = vectorize_df_ordered_cols(db2[d_name_map_c1c2.values()]).astype(np.float32)\n",
    "clock.tack('time_vectorize_db2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70100c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "04b24f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "clock.tick()\n",
    "M_rowsim = cosine_similarity(X_db1, X_db2)\n",
    "\n",
    "matching_row_in_db2 = M_rowsim.argmax(axis=1)\n",
    "matching_row_in_db2_sim = M_rowsim.max(axis=1)\n",
    "clock.tack('time_matching')\n",
    "\n",
    "db1['matching_row_in_db2'] = matching_row_in_db2\n",
    "db1['sim'] = matching_row_in_db2_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "92deac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "clock.tick()\n",
    "list_top5 = []\n",
    "for row in M_rowsim:\n",
    "    list_top5.append(row.argsort()[::-1][:5])\n",
    "    \n",
    "matching_row_in_db2_top5 = np.array(list_top5)\n",
    "clock.tack('time_argsort_for_topk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286eaa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf294f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1['matching_row_in_db2_top5'] = [row for row in matching_row_in_db2_top5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aa33e197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IR Store_code</th>\n",
       "      <th>CANAL SF</th>\n",
       "      <th>RANK NACIONAL</th>\n",
       "      <th>CODE</th>\n",
       "      <th>CHAIN SF</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>matching_row_in_db2</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5200016</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>MERCADONA, S.A.</td>\n",
       "      <td>CL DE LOS VELEZ, MARQUES SN CLCARLOS V</td>\n",
       "      <td>12</td>\n",
       "      <td>0.982882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5200014</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>374.0</td>\n",
       "      <td>SUPER 1-5</td>\n",
       "      <td>GRUPO EROSKI</td>\n",
       "      <td>CL CUARTEL VALENZUELA SN  C.C.PARQUE MELILLA</td>\n",
       "      <td>10</td>\n",
       "      <td>0.979956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5200007</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>LIDL SUPERMERCADOS, S.A.U.</td>\n",
       "      <td>CL MAANAN BENAISA MIMUM SN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.987224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5200001</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>1419.0</td>\n",
       "      <td>RESTO SUPER 51-184</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>CL GENERAL POLAVIEJA 30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.979883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5200008</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>GM FOOD IBERICA</td>\n",
       "      <td>CL FERNANDEZ CUEVAS 1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.981585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5200003</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>38.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>CL ALCALDE RAFAEL GINEL 12 EDIF. SAN LORENZO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.972946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5200009</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>6419.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>CL MARQUES DE MONTEMAR 5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.991414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5200006</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>COVIRAN, S.C.A.</td>\n",
       "      <td>CL ALFONSO XIII SN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.968788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5200015</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>6889.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>CL GENERAL VILLALBA 48</td>\n",
       "      <td>11</td>\n",
       "      <td>0.985194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5200004</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>COVIRAN, S.C.A.</td>\n",
       "      <td>CL LAS MARGARITAS SN NAVE 1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.982281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5200011</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>7153.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>CL GARCIA CABRELLES 20</td>\n",
       "      <td>8</td>\n",
       "      <td>0.984537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5200012</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>7287.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>CL CABRERIZAS 17 PLANTA 7-PUERTA B</td>\n",
       "      <td>9</td>\n",
       "      <td>0.961630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5200002</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>744.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>CL MADRID SN EDIF. ZURBARAN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IR Store_code        CANAL SF RANK NACIONAL                CODE  \\\n",
       "0        5200016     CANAL NO SF           nan                 nan   \n",
       "1        5200014  SUPER NACIONAL         374.0           SUPER 1-5   \n",
       "2        5200007     CANAL NO SF           nan                 nan   \n",
       "3        5200001  SUPER REGIONAL        1419.0  RESTO SUPER 51-184   \n",
       "4        5200008     CANAL NO SF           nan                 nan   \n",
       "5        5200003  SUPER REGIONAL          38.0         RESTO SUPER   \n",
       "6        5200009  SUPER NACIONAL        6419.0         RESTO SUPER   \n",
       "7        5200006     CANAL NO SF           nan                 nan   \n",
       "8        5200015  SUPER NACIONAL        6889.0         RESTO SUPER   \n",
       "9        5200004     CANAL NO SF           nan                 nan   \n",
       "10       5200011  SUPER NACIONAL        7153.0         RESTO SUPER   \n",
       "11       5200012  SUPER NACIONAL        7287.0         RESTO SUPER   \n",
       "12       5200002  SUPER REGIONAL         744.0         RESTO SUPER   \n",
       "\n",
       "                      CHAIN SF                                        ADDRESS  \\\n",
       "0              MERCADONA, S.A.        CL DE LOS VELEZ, MARQUES SN CLCARLOS V    \n",
       "1                 GRUPO EROSKI   CL CUARTEL VALENZUELA SN  C.C.PARQUE MELILLA   \n",
       "2   LIDL SUPERMERCADOS, S.A.U.                   CL MAANAN BENAISA MIMUM SN     \n",
       "3         SUPERSOL SPAIN, S.L.                      CL GENERAL POLAVIEJA 30     \n",
       "4              GM FOOD IBERICA                        CL FERNANDEZ CUEVAS 1     \n",
       "5         SUPERSOL SPAIN, S.L.  CL ALCALDE RAFAEL GINEL 12 EDIF. SAN LORENZO    \n",
       "6                    DIA, S.A.                     CL MARQUES DE MONTEMAR 5     \n",
       "7              COVIRAN, S.C.A.                           CL ALFONSO XIII SN     \n",
       "8                    DIA, S.A.                       CL GENERAL VILLALBA 48     \n",
       "9              COVIRAN, S.C.A.                   CL LAS MARGARITAS SN NAVE 1    \n",
       "10                   DIA, S.A.                       CL GARCIA CABRELLES 20     \n",
       "11                   DIA, S.A.            CL CABRERIZAS 17 PLANTA 7-PUERTA B    \n",
       "12        SUPERSOL SPAIN, S.L.                   CL MADRID SN EDIF. ZURBARAN    \n",
       "\n",
       "    matching_row_in_db2  match_score  \n",
       "0                    12     0.982882  \n",
       "1                    10     0.979956  \n",
       "2                     5     0.987224  \n",
       "3                     0     0.979883  \n",
       "4                     6     0.981585  \n",
       "5                     2     0.972946  \n",
       "6                     7     0.991414  \n",
       "7                     4     0.968788  \n",
       "8                    11     0.985194  \n",
       "9                     3     0.982281  \n",
       "10                    8     0.984537  \n",
       "11                    9     0.961630  \n",
       "12                    1     0.951304  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1[['IR Store_code','CANAL SF','RANK NACIONAL','CODE','CHAIN SF','ADDRESS', 'matching_row_in_db2', 'sim']].rename(columns={'sim': 'match_score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "de5648b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODIGO</th>\n",
       "      <th>MACROCADENAS_NOMBRE</th>\n",
       "      <th>CADENA</th>\n",
       "      <th>ROTULO</th>\n",
       "      <th>ABRV</th>\n",
       "      <th>DIRECCION</th>\n",
       "      <th>NUMERO</th>\n",
       "      <th>DIRECCION1</th>\n",
       "      <th>DIRECCION_COD</th>\n",
       "      <th>COD_POSTAL</th>\n",
       "      <th>...</th>\n",
       "      <th>POT_8</th>\n",
       "      <th>POT_9</th>\n",
       "      <th>POT_10</th>\n",
       "      <th>POT_11</th>\n",
       "      <th>POT_12</th>\n",
       "      <th>POT_13</th>\n",
       "      <th>POT_14</th>\n",
       "      <th>POT_15</th>\n",
       "      <th>POINT_X</th>\n",
       "      <th>POINT_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5200001</td>\n",
       "      <td>EUROMADI IBERICA,S.A.</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>CL</td>\n",
       "      <td>GENERAL POLAVIEJA</td>\n",
       "      <td>30</td>\n",
       "      <td>nan</td>\n",
       "      <td>GENERAL POLAVIEJA 30_005200001</td>\n",
       "      <td>52006</td>\n",
       "      <td>...</td>\n",
       "      <td>7.65</td>\n",
       "      <td>7.62</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.74</td>\n",
       "      <td>5.06</td>\n",
       "      <td>1.19</td>\n",
       "      <td>9.56</td>\n",
       "      <td>6.28</td>\n",
       "      <td>-2.94225087774823</td>\n",
       "      <td>35.2847260582838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5200002</td>\n",
       "      <td>EUROMADI IBERICA,S.A.</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>CL</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>S/N</td>\n",
       "      <td>EDIF. ZURBARAN</td>\n",
       "      <td>MADRID S/N_005200002</td>\n",
       "      <td>52001</td>\n",
       "      <td>...</td>\n",
       "      <td>3.16</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.52</td>\n",
       "      <td>-2.95355215298406</td>\n",
       "      <td>35.2884665409159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5200003</td>\n",
       "      <td>EUROMADI IBERICA,S.A.</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>CL</td>\n",
       "      <td>ALCALDE RAFAEL GINEL</td>\n",
       "      <td>12</td>\n",
       "      <td>EDIF. SAN LORENZO</td>\n",
       "      <td>ALCALDE RAFAEL GINEL 12_005200003</td>\n",
       "      <td>52001</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.14</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.51</td>\n",
       "      <td>4.79</td>\n",
       "      <td>3.37</td>\n",
       "      <td>-2.93800895948979</td>\n",
       "      <td>35.2868764082751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5200004</td>\n",
       "      <td>EUROMADI IBERICA,S.A.</td>\n",
       "      <td>COVIRAN, S.C.A.</td>\n",
       "      <td>COVIRAN</td>\n",
       "      <td>CL</td>\n",
       "      <td>LAS MARGARITAS</td>\n",
       "      <td>S/N</td>\n",
       "      <td>NAVE 1</td>\n",
       "      <td>LAS MARGARITAS S/N_005200004</td>\n",
       "      <td>52006</td>\n",
       "      <td>...</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.72</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-2.94146830821349</td>\n",
       "      <td>35.2717502054199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5200006</td>\n",
       "      <td>EUROMADI IBERICA,S.A.</td>\n",
       "      <td>COVIRAN, S.C.A.</td>\n",
       "      <td>COVIRAN</td>\n",
       "      <td>CL</td>\n",
       "      <td>ALFONSO XIII</td>\n",
       "      <td>S/N</td>\n",
       "      <td>nan</td>\n",
       "      <td>ALFONSO XIII S/N_005200006</td>\n",
       "      <td>52006</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.99</td>\n",
       "      <td>1.73</td>\n",
       "      <td>-2.93561156317003</td>\n",
       "      <td>35.2933079020288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5200007</td>\n",
       "      <td>INDEPENDIENTE</td>\n",
       "      <td>LIDL SUPERMERCADOS, S.A.U.</td>\n",
       "      <td>LIDL</td>\n",
       "      <td>CL</td>\n",
       "      <td>MAANAN BENAISA MIMUM</td>\n",
       "      <td>S/N</td>\n",
       "      <td>nan</td>\n",
       "      <td>MAANAN BENAISA MIMUM S/N_005200007</td>\n",
       "      <td>52005</td>\n",
       "      <td>...</td>\n",
       "      <td>6.21</td>\n",
       "      <td>6.85</td>\n",
       "      <td>4.38</td>\n",
       "      <td>5.74</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.18</td>\n",
       "      <td>6.89</td>\n",
       "      <td>6.46</td>\n",
       "      <td>-2.95687136207697</td>\n",
       "      <td>35.2890075345388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5200008</td>\n",
       "      <td>EUROMADI IBERICA,S.A.</td>\n",
       "      <td>GM FOOD IBERICA</td>\n",
       "      <td>SUMA</td>\n",
       "      <td>CL</td>\n",
       "      <td>FERNANDEZ CUEVAS</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>FERNANDEZ CUEVAS 1_005200008</td>\n",
       "      <td>52005</td>\n",
       "      <td>...</td>\n",
       "      <td>12.65</td>\n",
       "      <td>3.22</td>\n",
       "      <td>7.53</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>10.92</td>\n",
       "      <td>4.92</td>\n",
       "      <td>-2.94764269703977</td>\n",
       "      <td>35.2899884593456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5200009</td>\n",
       "      <td>INDEPENDIENTE</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>DIA MARKET</td>\n",
       "      <td>CL</td>\n",
       "      <td>MARQUES DE MONTEMAR</td>\n",
       "      <td>5</td>\n",
       "      <td>nan</td>\n",
       "      <td>MARQUES DE MONTEMAR 5_005200009</td>\n",
       "      <td>52006</td>\n",
       "      <td>...</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.76</td>\n",
       "      <td>-2.94020053413355</td>\n",
       "      <td>35.2852018637255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5200011</td>\n",
       "      <td>INDEPENDIENTE</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>DIA MAXI</td>\n",
       "      <td>CL</td>\n",
       "      <td>GARCIA CABRELLES</td>\n",
       "      <td>20</td>\n",
       "      <td>nan</td>\n",
       "      <td>GARCIA CABRELLES 20_005200011</td>\n",
       "      <td>52002</td>\n",
       "      <td>...</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-2.94400726672052</td>\n",
       "      <td>35.2955264665589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5200012</td>\n",
       "      <td>INDEPENDIENTE</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>DIA MAXI</td>\n",
       "      <td>CL</td>\n",
       "      <td>CABRERIZAS</td>\n",
       "      <td>17</td>\n",
       "      <td>PLANTA 7-PUERTA B</td>\n",
       "      <td>CABRERIZAS 17_005200012</td>\n",
       "      <td>52203</td>\n",
       "      <td>...</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.54</td>\n",
       "      <td>-2.94846725093748</td>\n",
       "      <td>35.2966408020948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5200014</td>\n",
       "      <td>INDEPENDIENTE</td>\n",
       "      <td>GRUPO EROSKI</td>\n",
       "      <td>EROSKI</td>\n",
       "      <td>CL</td>\n",
       "      <td>CUARTEL VALENZUELA</td>\n",
       "      <td>S/N</td>\n",
       "      <td>nan</td>\n",
       "      <td>CUARTEL VALENZUELA S/N_005200014</td>\n",
       "      <td>52006</td>\n",
       "      <td>...</td>\n",
       "      <td>14.62</td>\n",
       "      <td>22.39</td>\n",
       "      <td>19.39</td>\n",
       "      <td>20.2</td>\n",
       "      <td>17.18</td>\n",
       "      <td>14.75</td>\n",
       "      <td>19.49</td>\n",
       "      <td>20.26</td>\n",
       "      <td>-2.93835971336619</td>\n",
       "      <td>35.2718604386139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5200015</td>\n",
       "      <td>INDEPENDIENTE</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>DIA MARKET</td>\n",
       "      <td>CL</td>\n",
       "      <td>GENERAL VILLALBA</td>\n",
       "      <td>48</td>\n",
       "      <td>nan</td>\n",
       "      <td>GENERAL VILLALBA 48_005200015</td>\n",
       "      <td>52002</td>\n",
       "      <td>...</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.64</td>\n",
       "      <td>-2.93772644881398</td>\n",
       "      <td>35.2735230077616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5200016</td>\n",
       "      <td>INDEPENDIENTE</td>\n",
       "      <td>MERCADONA, S.A.</td>\n",
       "      <td>MERCADONA</td>\n",
       "      <td>CL</td>\n",
       "      <td>DE LOS VELEZ, MARQUES</td>\n",
       "      <td>S/N</td>\n",
       "      <td>CL/CARLOS V</td>\n",
       "      <td>DE LOS VELEZ, MARQUES S/N_005200016</td>\n",
       "      <td>52001</td>\n",
       "      <td>...</td>\n",
       "      <td>25.3</td>\n",
       "      <td>17.98</td>\n",
       "      <td>20.05</td>\n",
       "      <td>22.17</td>\n",
       "      <td>20.96</td>\n",
       "      <td>13.56</td>\n",
       "      <td>30.18</td>\n",
       "      <td>21.04</td>\n",
       "      <td>-2.93931715510028</td>\n",
       "      <td>35.2816429437493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CODIGO    MACROCADENAS_NOMBRE                      CADENA      ROTULO  \\\n",
       "0   5200001  EUROMADI IBERICA,S.A.        SUPERSOL SPAIN, S.L.    SUPERSOL   \n",
       "1   5200002  EUROMADI IBERICA,S.A.        SUPERSOL SPAIN, S.L.    SUPERSOL   \n",
       "2   5200003  EUROMADI IBERICA,S.A.        SUPERSOL SPAIN, S.L.    SUPERSOL   \n",
       "3   5200004  EUROMADI IBERICA,S.A.             COVIRAN, S.C.A.     COVIRAN   \n",
       "4   5200006  EUROMADI IBERICA,S.A.             COVIRAN, S.C.A.     COVIRAN   \n",
       "5   5200007          INDEPENDIENTE  LIDL SUPERMERCADOS, S.A.U.        LIDL   \n",
       "6   5200008  EUROMADI IBERICA,S.A.             GM FOOD IBERICA        SUMA   \n",
       "7   5200009          INDEPENDIENTE                   DIA, S.A.  DIA MARKET   \n",
       "8   5200011          INDEPENDIENTE                   DIA, S.A.    DIA MAXI   \n",
       "9   5200012          INDEPENDIENTE                   DIA, S.A.    DIA MAXI   \n",
       "10  5200014          INDEPENDIENTE                GRUPO EROSKI      EROSKI   \n",
       "11  5200015          INDEPENDIENTE                   DIA, S.A.  DIA MARKET   \n",
       "12  5200016          INDEPENDIENTE             MERCADONA, S.A.   MERCADONA   \n",
       "\n",
       "   ABRV              DIRECCION NUMERO         DIRECCION1  \\\n",
       "0    CL      GENERAL POLAVIEJA     30                nan   \n",
       "1    CL                 MADRID    S/N     EDIF. ZURBARAN   \n",
       "2    CL   ALCALDE RAFAEL GINEL     12  EDIF. SAN LORENZO   \n",
       "3    CL         LAS MARGARITAS    S/N             NAVE 1   \n",
       "4    CL           ALFONSO XIII    S/N                nan   \n",
       "5    CL   MAANAN BENAISA MIMUM    S/N                nan   \n",
       "6    CL       FERNANDEZ CUEVAS      1                nan   \n",
       "7    CL    MARQUES DE MONTEMAR      5                nan   \n",
       "8    CL       GARCIA CABRELLES     20                nan   \n",
       "9    CL             CABRERIZAS     17  PLANTA 7-PUERTA B   \n",
       "10   CL     CUARTEL VALENZUELA    S/N                nan   \n",
       "11   CL       GENERAL VILLALBA     48                nan   \n",
       "12   CL  DE LOS VELEZ, MARQUES    S/N        CL/CARLOS V   \n",
       "\n",
       "                          DIRECCION_COD COD_POSTAL  ...  POT_8  POT_9 POT_10  \\\n",
       "0        GENERAL POLAVIEJA 30_005200001      52006  ...   7.65   7.62    6.3   \n",
       "1                  MADRID S/N_005200002      52001  ...   3.16   1.82   1.19   \n",
       "2     ALCALDE RAFAEL GINEL 12_005200003      52001  ...    4.4   4.14   3.46   \n",
       "3          LAS MARGARITAS S/N_005200004      52006  ...   3.06   0.87   1.88   \n",
       "4            ALFONSO XIII S/N_005200006      52006  ...    3.3    1.1   2.15   \n",
       "5    MAANAN BENAISA MIMUM S/N_005200007      52005  ...   6.21   6.85   4.38   \n",
       "6          FERNANDEZ CUEVAS 1_005200008      52005  ...  12.65   3.22   7.53   \n",
       "7       MARQUES DE MONTEMAR 5_005200009      52006  ...   3.34    1.7   1.71   \n",
       "8         GARCIA CABRELLES 20_005200011      52002  ...   2.29   1.68   1.77   \n",
       "9               CABRERIZAS 17_005200012      52203  ...   2.23   1.65   1.74   \n",
       "10     CUARTEL VALENZUELA S/N_005200014      52006  ...  14.62  22.39  19.39   \n",
       "11        GENERAL VILLALBA 48_005200015      52002  ...   3.09   1.57   1.61   \n",
       "12  DE LOS VELEZ, MARQUES S/N_005200016      52001  ...   25.3  17.98  20.05   \n",
       "\n",
       "   POT_11 POT_12 POT_13 POT_14 POT_15            POINT_X           POINT_Y  \n",
       "0    6.74   5.06   1.19   9.56   6.28  -2.94225087774823  35.2847260582838  \n",
       "1    1.17   1.01   0.15    2.7   1.52  -2.95355215298406  35.2884665409159  \n",
       "2     3.6   2.86   0.51   4.79   3.37  -2.93800895948979  35.2868764082751  \n",
       "3    0.57   1.06   0.04   3.72   1.57  -2.94146830821349  35.2717502054199  \n",
       "4    0.64   1.24   0.03   3.99   1.73  -2.93561156317003  35.2933079020288  \n",
       "5    5.74   4.05   4.18   6.89   6.46  -2.95687136207697  35.2890075345388  \n",
       "6    3.51   2.96   0.02  10.92   4.92  -2.94764269703977  35.2899884593456  \n",
       "7    1.35   1.43   0.35    2.8   1.76  -2.94020053413355  35.2852018637255  \n",
       "8    0.76   1.48   0.23   2.09   1.57  -2.94400726672052  35.2955264665589  \n",
       "9    0.75   1.47   0.23   2.05   1.54  -2.94846725093748  35.2966408020948  \n",
       "10   20.2  17.18  14.75  19.49  20.26  -2.93835971336619  35.2718604386139  \n",
       "11   1.29   1.31   0.34   2.64   1.64  -2.93772644881398  35.2735230077616  \n",
       "12  22.17  20.96  13.56  30.18  21.04  -2.93931715510028  35.2816429437493  \n",
       "\n",
       "[13 rows x 37 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9787f91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IR Store_code</th>\n",
       "      <th>CANAL SF</th>\n",
       "      <th>RANK NACIONAL</th>\n",
       "      <th>CODE</th>\n",
       "      <th>CHAIN SF</th>\n",
       "      <th>ROTULO</th>\n",
       "      <th>BANNER CONTROL</th>\n",
       "      <th>BANNER SF (COMPILADO)</th>\n",
       "      <th>NAME STORE (ID)</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>CCAA</th>\n",
       "      <th>PROVINCIA</th>\n",
       "      <th>MUNICIPIO</th>\n",
       "      <th>SUPERFN</th>\n",
       "      <th>TIPO_NIELSEN_NEW</th>\n",
       "      <th>CHANNEL P&amp;G</th>\n",
       "      <th>AGR.2019 (P_15)</th>\n",
       "      <th>COBERTURA FY2122</th>\n",
       "      <th>matching_row_in_db2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5200016</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>MERCADONA, S.A.</td>\n",
       "      <td>MERCADONA</td>\n",
       "      <td>MERCADONA</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL DE LOS VELEZ, MARQUES SN CLCARLOS V</td>\n",
       "      <td>52001</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>1925</td>\n",
       "      <td>SUPER1 800-2499</td>\n",
       "      <td>DISCOUNTER</td>\n",
       "      <td>21.04</td>\n",
       "      <td>NO</td>\n",
       "      <td>12</td>\n",
       "      <td>0.982882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5200014</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>374.0</td>\n",
       "      <td>SUPER 1-5</td>\n",
       "      <td>GRUPO EROSKI</td>\n",
       "      <td>EROSKI</td>\n",
       "      <td>EROSKI SUPER</td>\n",
       "      <td>EROSKI SUPER</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL CUARTEL VALENZUELA SN  C.C.PARQUE MELILLA</td>\n",
       "      <td>52006</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>4200</td>\n",
       "      <td>HIPER2 2500-6499</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>20.26</td>\n",
       "      <td>NO</td>\n",
       "      <td>10</td>\n",
       "      <td>0.979956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5200007</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>LIDL SUPERMERCADOS, S.A.U.</td>\n",
       "      <td>LIDL</td>\n",
       "      <td>LIDL</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL MAANAN BENAISA MIMUM SN</td>\n",
       "      <td>52005</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>1400</td>\n",
       "      <td>SUPER1 800-2499</td>\n",
       "      <td>DISCOUNTER</td>\n",
       "      <td>6.46</td>\n",
       "      <td>NO</td>\n",
       "      <td>5</td>\n",
       "      <td>0.987224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5200001</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>1419.0</td>\n",
       "      <td>RESTO SUPER 51-184</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL GENERAL POLAVIEJA 30</td>\n",
       "      <td>52006</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>2483</td>\n",
       "      <td>SUPER1 800-2499</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>6.28</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0.979883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5200008</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>GM FOOD IBERICA</td>\n",
       "      <td>SUMA</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL FERNANDEZ CUEVAS 1</td>\n",
       "      <td>52005</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>1480</td>\n",
       "      <td>SUPER1 800-2499</td>\n",
       "      <td>HFS</td>\n",
       "      <td>4.92</td>\n",
       "      <td>NO</td>\n",
       "      <td>6</td>\n",
       "      <td>0.981585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5200003</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>38.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL ALCALDE RAFAEL GINEL 12 EDIF. SAN LORENZO</td>\n",
       "      <td>52001</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>1084</td>\n",
       "      <td>SUPER1 800-2499</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>3.37</td>\n",
       "      <td>NO</td>\n",
       "      <td>2</td>\n",
       "      <td>0.972946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5200009</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>6419.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>DIA MARKET</td>\n",
       "      <td>DIA</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL MARQUES DE MONTEMAR 5</td>\n",
       "      <td>52006</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>533</td>\n",
       "      <td>SUPER2 300-799</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>1.76</td>\n",
       "      <td>NO</td>\n",
       "      <td>7</td>\n",
       "      <td>0.991414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5200006</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>COVIRAN, S.C.A.</td>\n",
       "      <td>COVIRAN</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL ALFONSO XIII SN</td>\n",
       "      <td>52006</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>566</td>\n",
       "      <td>SUPER2 300-799</td>\n",
       "      <td>HFS</td>\n",
       "      <td>1.73</td>\n",
       "      <td>NO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.968788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5200015</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>6889.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>DIA MARKET</td>\n",
       "      <td>DIA</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL GENERAL VILLALBA 48</td>\n",
       "      <td>52002</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>481</td>\n",
       "      <td>SUPER2 300-799</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>1.64</td>\n",
       "      <td>NO</td>\n",
       "      <td>11</td>\n",
       "      <td>0.985194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5200004</td>\n",
       "      <td>CANAL NO SF</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>COVIRAN, S.C.A.</td>\n",
       "      <td>COVIRAN</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL LAS MARGARITAS SN NAVE 1</td>\n",
       "      <td>52006</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>466</td>\n",
       "      <td>SUPER2 300-799</td>\n",
       "      <td>HFS</td>\n",
       "      <td>1.57</td>\n",
       "      <td>NO</td>\n",
       "      <td>3</td>\n",
       "      <td>0.982281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5200011</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>7153.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>DIA MAXI</td>\n",
       "      <td>DIA</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL GARCIA CABRELLES 20</td>\n",
       "      <td>52002</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>268</td>\n",
       "      <td>SUPER3 &lt;300 ORG</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>1.57</td>\n",
       "      <td>NO</td>\n",
       "      <td>8</td>\n",
       "      <td>0.984537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5200012</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>7287.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>DIA, S.A.</td>\n",
       "      <td>DIA MAXI</td>\n",
       "      <td>DIA</td>\n",
       "      <td>BANNER NO SF</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL CABRERIZAS 17 PLANTA 7-PUERTA B</td>\n",
       "      <td>52203</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>251</td>\n",
       "      <td>SUPER3 &lt;300 ORG</td>\n",
       "      <td>SUPER NACIONAL</td>\n",
       "      <td>1.54</td>\n",
       "      <td>NO</td>\n",
       "      <td>9</td>\n",
       "      <td>0.961630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5200002</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>744.0</td>\n",
       "      <td>RESTO SUPER</td>\n",
       "      <td>SUPERSOL SPAIN, S.L.</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>SUPERSOL</td>\n",
       "      <td>STORE NO SF</td>\n",
       "      <td>CL MADRID SN EDIF. ZURBARAN</td>\n",
       "      <td>52001</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>MELILLA</td>\n",
       "      <td>335</td>\n",
       "      <td>SUPER2 300-799</td>\n",
       "      <td>SUPER REGIONAL</td>\n",
       "      <td>1.52</td>\n",
       "      <td>NO</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IR Store_code        CANAL SF RANK NACIONAL                CODE  \\\n",
       "0        5200016     CANAL NO SF           nan                 nan   \n",
       "1        5200014  SUPER NACIONAL         374.0           SUPER 1-5   \n",
       "2        5200007     CANAL NO SF           nan                 nan   \n",
       "3        5200001  SUPER REGIONAL        1419.0  RESTO SUPER 51-184   \n",
       "4        5200008     CANAL NO SF           nan                 nan   \n",
       "5        5200003  SUPER REGIONAL          38.0         RESTO SUPER   \n",
       "6        5200009  SUPER NACIONAL        6419.0         RESTO SUPER   \n",
       "7        5200006     CANAL NO SF           nan                 nan   \n",
       "8        5200015  SUPER NACIONAL        6889.0         RESTO SUPER   \n",
       "9        5200004     CANAL NO SF           nan                 nan   \n",
       "10       5200011  SUPER NACIONAL        7153.0         RESTO SUPER   \n",
       "11       5200012  SUPER NACIONAL        7287.0         RESTO SUPER   \n",
       "12       5200002  SUPER REGIONAL         744.0         RESTO SUPER   \n",
       "\n",
       "                      CHAIN SF      ROTULO BANNER CONTROL  \\\n",
       "0              MERCADONA, S.A.   MERCADONA      MERCADONA   \n",
       "1                 GRUPO EROSKI      EROSKI   EROSKI SUPER   \n",
       "2   LIDL SUPERMERCADOS, S.A.U.        LIDL           LIDL   \n",
       "3         SUPERSOL SPAIN, S.L.    SUPERSOL       SUPERSOL   \n",
       "4              GM FOOD IBERICA        SUMA   BANNER NO SF   \n",
       "5         SUPERSOL SPAIN, S.L.    SUPERSOL       SUPERSOL   \n",
       "6                    DIA, S.A.  DIA MARKET            DIA   \n",
       "7              COVIRAN, S.C.A.     COVIRAN   BANNER NO SF   \n",
       "8                    DIA, S.A.  DIA MARKET            DIA   \n",
       "9              COVIRAN, S.C.A.     COVIRAN   BANNER NO SF   \n",
       "10                   DIA, S.A.    DIA MAXI            DIA   \n",
       "11                   DIA, S.A.    DIA MAXI            DIA   \n",
       "12        SUPERSOL SPAIN, S.L.    SUPERSOL       SUPERSOL   \n",
       "\n",
       "   BANNER SF (COMPILADO) NAME STORE (ID)  \\\n",
       "0           BANNER NO SF     STORE NO SF   \n",
       "1           EROSKI SUPER     STORE NO SF   \n",
       "2           BANNER NO SF     STORE NO SF   \n",
       "3               SUPERSOL     STORE NO SF   \n",
       "4           BANNER NO SF     STORE NO SF   \n",
       "5               SUPERSOL     STORE NO SF   \n",
       "6           BANNER NO SF     STORE NO SF   \n",
       "7           BANNER NO SF     STORE NO SF   \n",
       "8           BANNER NO SF     STORE NO SF   \n",
       "9           BANNER NO SF     STORE NO SF   \n",
       "10          BANNER NO SF     STORE NO SF   \n",
       "11          BANNER NO SF     STORE NO SF   \n",
       "12              SUPERSOL     STORE NO SF   \n",
       "\n",
       "                                          ADDRESS    ZIP     CCAA PROVINCIA  \\\n",
       "0         CL DE LOS VELEZ, MARQUES SN CLCARLOS V   52001  MELILLA   MELILLA   \n",
       "1    CL CUARTEL VALENZUELA SN  C.C.PARQUE MELILLA  52006  MELILLA   MELILLA   \n",
       "2                    CL MAANAN BENAISA MIMUM SN    52005  MELILLA   MELILLA   \n",
       "3                       CL GENERAL POLAVIEJA 30    52006  MELILLA   MELILLA   \n",
       "4                         CL FERNANDEZ CUEVAS 1    52005  MELILLA   MELILLA   \n",
       "5   CL ALCALDE RAFAEL GINEL 12 EDIF. SAN LORENZO   52001  MELILLA   MELILLA   \n",
       "6                      CL MARQUES DE MONTEMAR 5    52006  MELILLA   MELILLA   \n",
       "7                            CL ALFONSO XIII SN    52006  MELILLA   MELILLA   \n",
       "8                        CL GENERAL VILLALBA 48    52002  MELILLA   MELILLA   \n",
       "9                    CL LAS MARGARITAS SN NAVE 1   52006  MELILLA   MELILLA   \n",
       "10                       CL GARCIA CABRELLES 20    52002  MELILLA   MELILLA   \n",
       "11            CL CABRERIZAS 17 PLANTA 7-PUERTA B   52203  MELILLA   MELILLA   \n",
       "12                   CL MADRID SN EDIF. ZURBARAN   52001  MELILLA   MELILLA   \n",
       "\n",
       "   MUNICIPIO SUPERFN  TIPO_NIELSEN_NEW     CHANNEL P&G AGR.2019 (P_15)  \\\n",
       "0    MELILLA    1925   SUPER1 800-2499      DISCOUNTER           21.04   \n",
       "1    MELILLA    4200  HIPER2 2500-6499  SUPER NACIONAL           20.26   \n",
       "2    MELILLA    1400   SUPER1 800-2499      DISCOUNTER            6.46   \n",
       "3    MELILLA    2483   SUPER1 800-2499  SUPER REGIONAL            6.28   \n",
       "4    MELILLA    1480   SUPER1 800-2499           HFS              4.92   \n",
       "5    MELILLA    1084   SUPER1 800-2499  SUPER REGIONAL            3.37   \n",
       "6    MELILLA     533    SUPER2 300-799  SUPER NACIONAL            1.76   \n",
       "7    MELILLA     566    SUPER2 300-799           HFS              1.73   \n",
       "8    MELILLA     481    SUPER2 300-799  SUPER NACIONAL            1.64   \n",
       "9    MELILLA     466    SUPER2 300-799           HFS              1.57   \n",
       "10   MELILLA     268   SUPER3 <300 ORG  SUPER NACIONAL            1.57   \n",
       "11   MELILLA     251   SUPER3 <300 ORG  SUPER NACIONAL            1.54   \n",
       "12   MELILLA     335    SUPER2 300-799  SUPER REGIONAL            1.52   \n",
       "\n",
       "   COBERTURA FY2122  matching_row_in_db2       sim  \n",
       "0                NO                   12  0.982882  \n",
       "1                NO                   10  0.979956  \n",
       "2                NO                    5  0.987224  \n",
       "3                NO                    0  0.979883  \n",
       "4                NO                    6  0.981585  \n",
       "5                NO                    2  0.972946  \n",
       "6                NO                    7  0.991414  \n",
       "7                NO                    4  0.968788  \n",
       "8                NO                   11  0.985194  \n",
       "9                NO                    3  0.982281  \n",
       "10               NO                    8  0.984537  \n",
       "11               NO                    9  0.961630  \n",
       "12               NO                    1  0.951304  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_columns', None):\n",
    "    display(db1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181e04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "026d8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db1[list(d_c1c2.keys())+['matching_row_in_db2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43b1c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slide: Not noisy vs correspondent noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24fef1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruido2: Falta algun campo\n",
    "# Ruido3: Swap campo\n",
    "# Rendimiento SCANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1c047512",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['company', 'address', 'name'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_286/393033291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m df_row_matches = pd.merge(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mleft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'matching_row_in_db2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'company'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'address'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'matching_row_in_db2_top5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'level_0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_name_map_c1c2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3461\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['company', 'address', 'name'] not in index\""
     ]
    }
   ],
   "source": [
    "df_row_matches = pd.merge(\n",
    "    left=db1.reset_index()[['index', 'matching_row_in_db2', 'company', 'address', 'name', 'sim', 'matching_row_in_db2_top5']],\n",
    "    right=db2.reset_index()[['level_0', 'index'] + list(d_name_map_c1c2.values())],\n",
    "    left_on='index',\n",
    "    right_on='index'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a631a2c",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4844d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_html_file_for_mlflow(df, path_artifact):\n",
    "    # Also, write HTML and log \n",
    "    from pretty_html_table import build_table\n",
    "    with pd.option_context(\"display.precision\", 4):\n",
    "        html_df = build_table(df, index=True, color='grey_light', font_family='Arial', font_size=12)\n",
    "        with open(path_artifact, 'w') as fb:\n",
    "            fb.write(html_df)\n",
    "            \n",
    "            \n",
    "dir_artifacts = Path('output') # Can it be temp?\n",
    "dir_artifacts.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1b065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "652d29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b34be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top1 acc - no thr\n",
    "df_row_matches['high_conf'] = df_row_matches['sim'] > 0\n",
    "\n",
    "df_row_matches['match_correct'] = df_row_matches['matching_row_in_db2'] == df_row_matches['level_0']\n",
    "df_ct = pd.crosstab(df_row_matches['high_conf'], df_row_matches['match_correct'], normalize='all')\n",
    "d_metrics['top1 acc'] = df_ct[True][True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "304d35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top1 acc - thr\n",
    "THR_CONFIDENCE_QUANTILE = 0.3 # Example of threshold threshold tests → We will not process bottom 30% confident rows\n",
    "# THR LEAVING OUT % OF SAMPLES\n",
    "THR = df_row_matches['sim'].quantile(THR_CONFIDENCE_QUANTILE)\n",
    "df_row_matches['high_conf'] = df_row_matches['sim'] > THR\n",
    "df_ct = pd.crosstab(df_row_matches['high_conf'], df_row_matches['match_correct'], normalize='index')\n",
    "d_metrics[f'top1 acc of processed'] = df_ct[True][True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ae778e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm this by 'all' instead of 'index'\n",
    "df_ct_for_barplot = pd.crosstab(df_row_matches['high_conf'], df_row_matches['match_correct'], normalize='all', dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bb98741",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_html_file_for_mlflow(df_ct.reset_index(), dir_artifacts / 'error_rate_thr_norm_row.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29536fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_html_file_for_mlflow(df_ct_for_barplot.reset_index(), dir_artifacts / 'error_rate_thr_norm_all.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dda9cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ct_for_barplot\n",
    "d_metrics_thr = {\n",
    "    'perc_processed_ok': df_ct_for_barplot[True][True],\n",
    "    \n",
    "    'perc_processed_ko': df_ct_for_barplot[False][True],\n",
    "    \n",
    "    'perc_not_processed': THR_CONFIDENCE_QUANTILE,\n",
    "}\n",
    "df_metrics_thr = pd.DataFrame(d_metrics_thr.items())\n",
    "\n",
    "df_metrics_thr[' '] = ''\n",
    "df_metrics_thr[1] = (df_metrics_thr[1]*100).apply(lambda x: round(x,2))\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.bar(\n",
    "    df_metrics_thr,\n",
    "    x=' ',\n",
    "    y=1,\n",
    "    color=0,\n",
    "    color_discrete_map={\n",
    "        'perc_not_processed': 'rgba(70,70,70,0.5)',\n",
    "        'perc_processed_ok': 'rgba(0,200,120,1)',\n",
    "        'perc_processed_ko': 'red'\n",
    "    },\n",
    "    text=1,\n",
    "    width=400,\n",
    "    \n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    uniformtext_minsize=12,\n",
    "    uniformtext_mode='hide',\n",
    "    legend_title_text='Case',\n",
    "    xaxis_title=\"\",\n",
    "    yaxis_title=\"%\",\n",
    "    title='Percentages of not processed, processed correctly and<br>processed incorrectly when using threshold',\n",
    ")\n",
    "fig.update_layout(title_font_size=12)\n",
    "\n",
    "fig.write_html(dir_artifacts / 'perc_not_proc_proc_ok_ko.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da698d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3f671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f67e7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top3 acc\n",
    "df_row_matches['match_correct_top3'] = df_row_matches.apply(lambda row: row['level_0'] in row['matching_row_in_db2_top5'][:3], axis=1)\n",
    "df_ct = pd.crosstab(df_row_matches['high_conf'], df_row_matches['match_correct_top3'], normalize='index')\n",
    "d_metrics['top3 acc'] = df_ct[True][True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d269d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78e67dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top5 acc\n",
    "df_row_matches['match_correct_top5'] = df_row_matches.apply(lambda row: row['level_0'] in row['matching_row_in_db2_top5'], axis=1)\n",
    "df_ct = pd.crosstab(df_row_matches['high_conf'], df_row_matches['match_correct_top5'], normalize='index')\n",
    "d_metrics['top5 acc'] = df_ct[True][True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27921a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426559b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe306f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f763010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875c449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc8eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4606de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d80482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "635c6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb\n",
    "# import scann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f872fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9ecbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searcher = scann.scann_ops_pybind.builder(X_db1, 10, \"dot_product\").tree(\n",
    "#     num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(\n",
    "#     2, anisotropic_quantization_threshold=0.2).reorder(100).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3f1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9de8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d4f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d35f6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>residence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3897</th>\n",
       "      <td>5405 Jessica Grove\\nNorth Matthew, MD 73186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15719</th>\n",
       "      <td>3205 Bowman Isle Suite 454\\nLake David, IN 66600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17915</th>\n",
       "      <td>354 Allen Fort Apt. 673\\nHayestown, CA 53217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12783</th>\n",
       "      <td>92597 Annette Branch\\nYoungchester, MO 29107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6880</th>\n",
       "      <td>4177 Sarah Pass\\nJacksonview, ID 21177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15785</th>\n",
       "      <td>PSC 3693, Box 9492\\nAPO AE 88261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>57520 Costa Hill Suite 261\\nLake Julieborough, HI 65335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     residence\n",
       "3897               5405 Jessica Grove\\nNorth Matthew, MD 73186\n",
       "15719         3205 Bowman Isle Suite 454\\nLake David, IN 66600\n",
       "17915             354 Allen Fort Apt. 673\\nHayestown, CA 53217\n",
       "12783             92597 Annette Branch\\nYoungchester, MO 29107\n",
       "6880                    4177 Sarah Pass\\nJacksonview, ID 21177\n",
       "15785                         PSC 3693, Box 9492\\nAPO AE 88261\n",
       "2426   57520 Costa Hill Suite 261\\nLake Julieborough, HI 65335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_rows = db1.sample(7)\n",
    "with pd.option_context('max.colwidth', None):\n",
    "    display(sample_rows[['residence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e2e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88d1cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example libpostal\n",
    "# from postal.parser import parse_address\n",
    "# from postal.expand import expand_address\n",
    "# db1_residences_parsed = sample_rows[['residence']]\n",
    "# db1_residences_parsed['residence'] = db1_residences_parsed['residence'].apply(\n",
    "#     lambda txt: dict([(v,k) for k,v in parse_address(txt)]))\n",
    "# pd.json_normalize(db1_residences_parsed['residence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f2681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581f55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2945a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(\n",
    "    df_row_matches['sim'],\n",
    "    color=df_row_matches['match_correct'],\n",
    "    barmode='stack'\n",
    ")\n",
    "fig.add_vline(x=THR, line_dash='dash')\n",
    "fig.update_layout(\n",
    "    legend_title_text='Match correct',\n",
    "    xaxis_title=\"Confidence\",\n",
    "    yaxis_title=\"Num matched elements\",\n",
    "    title='Error rate by confidence interval'\n",
    ")\n",
    "fig.write_html(dir_artifacts / 'error_rate_by_conf.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0801d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    for k in [\n",
    "        'NUM_TOKENS_KEPT_ADDR',\n",
    "        'P_NOISE_CHAR',\n",
    "        'ROW_COL_MISSING_OR_SWAPPED',\n",
    "        'P_ROW_COL_MISSING_OR_SWAPPED',\n",
    "        'FRAC_KEPT_ROWS_DB2',\n",
    "        'TFIDF_ANALYZER',\n",
    "        'TFIDF_NGRAM_LO',\n",
    "        'TFIDF_NGRAM_HI',\n",
    "        'TFIDF_MAX_DF',\n",
    "        'TFIDF_MIN_DF',\n",
    "        'TFIDF_MAX_FEATS',\n",
    "        'TFIDF_VOCAB',\n",
    "        'THR_CONFIDENCE_QUANTILE',\n",
    "        'THR',\n",
    "    ]:\n",
    "        mlflow.log_param(k, locals()[k])\n",
    "    \n",
    "    for k, v in d_metrics.items():\n",
    "        mlflow.log_metric(k, v)\n",
    "        \n",
    "    for k, v in clock.d.items():\n",
    "        mlflow.log_metric(k, v)\n",
    "    \n",
    "    mlflow.log_artifacts(dir_artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65efe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8750d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7ded2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
